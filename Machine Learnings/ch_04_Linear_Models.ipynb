{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "A linear model is a mathematical representation of the relationship between input variables (features) and an output variable, assuming a linear relationship. In a linear model, the output is modeled as a weighted sum of the input features, possibly with an additional constant term known as the bias or intercept. Linear models are widely used in both regression and classification tasks in machine learning.\n",
    "\n",
    "Here are some common types of linear models, each with its own characteristics and use cases:\n",
    "\n",
    "### **1. Linear Regression:**\n",
    "\n",
    "Linear regression is a regression algorithm used for predicting a continuous output variable based on one or more input features. The linear regression model is expressed as:\n",
    "\n",
    "\\[ y = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n + \\varepsilon \\]\n",
    "\n",
    "- \\(y\\) is the predicted output.\n",
    "- \\(w_0\\) is the intercept (bias term).\n",
    "- \\(w_1, w_2, \\ldots, w_n\\) are the coefficients for the input features \\(x_1, x_2, \\ldots, x_n\\).\n",
    "- \\(\\varepsilon\\) represents the error term.\n",
    "\n",
    "The objective of linear regression is to find the optimal values for the coefficients that minimize the sum of squared differences between the predicted and actual output values.\n",
    "\n",
    "**Example Application:** Predicting house prices based on features like square footage, number of bedrooms, etc.\n",
    "\n",
    "### **2. Ridge Regression:**\n",
    "\n",
    "Ridge regression is an extension of linear regression that includes regularization to prevent overfitting. It adds a regularization term to the linear regression cost function, penalizing large coefficients:\n",
    "\n",
    "\\[ \\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{m} w_j^2 \\]\n",
    "\n",
    "- \\(y_i\\) is the actual output for the \\(i\\)-th instance.\n",
    "- \\(\\hat{y}_i\\) is the predicted output for the \\(i\\)-th instance.\n",
    "- \\(w_j\\) are the coefficients.\n",
    "- \\(\\alpha\\) controls the strength of regularization.\n",
    "\n",
    "**Example Application:** Similar to linear regression but with added regularization to handle multicollinearity.\n",
    "\n",
    "### **3. Lasso Regression:**\n",
    "\n",
    "Lasso regression, like Ridge regression, is a regularization technique applied to linear regression. It adds a regularization term that penalizes the absolute values of the coefficients:\n",
    "\n",
    "\\[ \\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{m} |w_j| \\]\n",
    "\n",
    "- \\(\\alpha\\) controls the strength of regularization.\n",
    "\n",
    "Lasso regression tends to produce sparse models, effectively performing feature selection by driving some coefficients to exactly zero.\n",
    "\n",
    "**Example Application:** Feature selection in cases where some features are expected to have negligible impact.\n",
    "\n",
    "### **4. Logistic Regression:**\n",
    "\n",
    "Logistic regression is a linear model used for binary classification tasks. The logistic regression model is expressed as:\n",
    "\n",
    "\\[ P(Y=1) = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n)}} \\]\n",
    "\n",
    "- \\(P(Y=1)\\) is the probability of belonging to class 1.\n",
    "- \\(e\\) is the base of the natural logarithm.\n",
    "- \\(w_0, w_1, \\ldots, w_n\\) are the coefficients for the input features \\(x_1, x_2, \\ldots, x_n\\).\n",
    "\n",
    "The logistic function (sigmoid) is used to map the linear combination of features to a probability value, and thresholding is applied to make binary predictions.\n",
    "\n",
    "**Example Application:** Predicting whether an email is spam or not spam.\n",
    "\n",
    "### **5. Elastic Net Regression:**\n",
    "\n",
    "Elastic Net is a linear regression model that combines L1 (Lasso) and L2 (Ridge) regularization terms in the cost function. It includes both penalty terms, allowing for both feature selection and coefficient shrinkage:\n",
    "\n",
    "\\[ \\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\rho \\sum_{j=1}^{m} |w_j| + \\frac{\\alpha (1-\\rho)}{2} \\sum_{j=1}^{m} w_j^2 \\]\n",
    "\n",
    "- \\(\\alpha\\) controls the overall strength of regularization.\n",
    "- \\(\\rho\\) balances the contributions of L1 and L2 regularization.\n",
    "\n",
    "**Example Application:** Similar to linear regression with the ability to handle feature selection and multicollinearity.\n",
    "\n",
    "### **6. Polynomial Regression:**\n",
    "\n",
    "Polynomial regression is an extension of linear regression that allows for modeling non-linear relationships by introducing polynomial terms. The polynomial regression equation is expressed as:\n",
    "\n",
    "\\[ y = w_0 + w_1x + w_2x^2 + \\ldots + w_dx^d + \\varepsilon \\]\n",
    "\n",
    "- \\(x\\) is the input feature.\n",
    "- \\(d\\) is the degree of the polynomial.\n",
    "\n",
    "Polynomial regression can capture more complex patterns in the data but may be prone to overfitting.\n",
    "\n",
    "**Example Application:** Predicting a variable that exhibits a curvilinear relationship with the input.\n",
    "\n",
    "### **7. Perceptron:**\n",
    "\n",
    "A perceptron is the simplest form of a neural network, functioning as a linear classifier for binary classification tasks. It takes a set of input features, applies weights to them, sums the weighted inputs, and passes the result through an activation function.\n",
    "\n",
    "\\[ \\text{Output} = \\text{Activation}(\\sum_{i=1}^{n} w_ix_i + b) \\]\n",
    "\n",
    "- \\(w_i\\) are the weights.\n",
    "- \\(x_i\\) are the input features.\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "Perceptrons are building blocks of more complex neural networks.\n",
    "\n",
    "**Example Application:** Binary classification tasks where linear separation is sufficient.\n",
    "\n",
    "### **8. Support Vector Machines (SVM):**\n",
    "\n",
    "Support Vector Machines can be considered linear models for classification tasks. SVM aims to find the hyperplane that best separates the data into different classes while maximizing the margin between the classes.\n",
    "\n",
    "\\[ w \\cdot x + b = 0 \\]\n",
    "\n",
    "- \\(w\\) is the weight vector.\n",
    "- \\(x\\) is the input features.\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "SVM is particularly effective in high-dimensional spaces.\n",
    "\n",
    "**Example Application:** Image classification, text classification.\n",
    "\n",
    "### **9. Generalized Linear Models (GLM):**\n",
    "\n",
    "Generalized Linear Models are a broad class of linear models that include linear regression, logistic regression, and more. GLMs extend the linear model to handle various types of response variables and error distributions.\n",
    "\n",
    "\\[ g(\\mu) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n \\]\n",
    "\n",
    "- \\(g(\\mu)\\) is the link function.\n",
    "- \\(\\mu\\) is the expected value of the response variable.\n",
    "\n",
    "GLMs can accommodate different probability distributions and are flexible in modeling diverse data types.\n",
    "\n",
    "**Example Application:** Predicting outcomes with non-constant variance or non-normal distributions.\n",
    "\n",
    "### **10. Bayesian Linear Regression:**\n",
    "\n",
    "Bayesian Linear Regression introduces a Bayesian framework to linear regression, allowing for the incorporation of prior beliefs about the model parameters. It provides a probability distribution over the model parameters.\n",
    "\n",
    "\\[ P(w | X, y) = \\frac{P(y | X, w) P(w)}{P(y | X)} \\]\n",
    "\n",
    "- \\(w\\) is the vector of model parameters.\n",
    "- \\(X\\) is the matrix of input features.\n",
    "- \\(y\\) is the vector of output values.\n",
    "\n",
    "Bayesian Linear Regression provides uncertainty estimates along with point predictions.\n",
    "\n",
    "**Example Application:** Incorporating prior knowledge into linear regression models.\n",
    "\n",
    "### **11. Robust Regression:**\n",
    "\n",
    "Robust regression models are designed to be less sensitive to outliers in the data compared to standard linear regression. They use different loss functions to minimize the impact of outliers on the model.\n",
    "\n",
    "- **Huber Regression:**\n",
    "  - Utilizes a combination of squared and absolute errors, providing a compromise between the mean squared error used in linear regression and the mean absolute error used in robust regression.\n",
    "  - Less sensitive to outliers than least squares regression.\n",
    "\n",
    "**Example Application:** Regression tasks where the presence of outliers can significantly impact model performance.\n",
    "\n",
    "### **12. Quantile Regression:**\n",
    "\n",
    "Quantile regression models the relationship between variables at different quantiles of the conditional distribution of the response variable. It is useful when the variability of the data is not constant across the entire range.\n",
    "\n",
    "\\[ Q_\\tau(y | X) = X\\beta_\\tau \\]\n",
    "\n",
    "- \\(Q_\\tau(y | X)\\) is the \\(\\tau\\)-th quantile of the response variable.\n",
    "- \\(\\beta_\\tau\\) is the vector of coefficients for the \\(\\tau\\)-th quantile.\n",
    "\n",
    "**Example Application:** Modeling the impact of predictors on different percentiles of income.\n",
    "\n",
    "### **13. Least Angle Regression (LARS):**\n",
    "\n",
    "LARS is a linear model that is particularly useful when dealing with a large number of predictors. It performs a stepwise forward selection of variables while keeping the coefficients as aligned as possible.\n",
    "\n",
    "**Example Application:** High-dimensional data sets where variable selection is crucial.\n",
    "\n",
    "### **14. Poisson Regression:**\n",
    "\n",
    "Poisson regression is used when the response variable is a count. It models the distribution of the response variable as a Poisson distribution.\n",
    "\n",
    "\\[ \\log(\\lambda) = X\\beta \\]\n",
    "\n",
    "- \\(\\lambda\\) is the mean of the Poisson distribution.\n",
    "- \\(X\\) is the matrix of input features.\n",
    "\n",
    "**Example Application:** Modeling the number of events in a fixed interval of time or space.\n",
    "\n",
    "### **15. Stepwise Regression:**\n",
    "\n",
    "Stepwise regression is an iterative method that selects the most significant variables to include in the model or removes the least significant variables. It can be forward, backward, or both.\n",
    "\n",
    "**Example Application:** Variable selection in cases with a large number of potential predictors.\n",
    "\n",
    "### **16. Piecewise Regression:**\n",
    "\n",
    "Piecewise regression models different linear relationships between variables for distinct ranges of the predictor variable. This approach is useful when there is evidence of different trends in different parts of the data.\n",
    "\n",
    "**Example Application:** Modeling a variable that exhibits different linear relationships in different intervals.\n",
    "\n",
    "These additional types of linear models provide specialized solutions to address specific challenges or characteristics of data. The choice of which model to use depends on the nature of the data and the assumptions underlying the modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models:\n",
    "Linear models can be broadly categorized into two main types based on the nature of the learning task: regression and classification. Here's a breakdown of linear models within each category:\n",
    "\n",
    "### **1. Linear Regression Models:**\n",
    "Linear regression models are used for predicting a continuous output variable. The general form of a linear regression equation is:\n",
    "\n",
    "\\[ y = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n + \\varepsilon \\]\n",
    "\n",
    "- **Ordinary Least Squares (OLS) Regression:** The most common form of linear regression that minimizes the sum of squared differences between predicted and actual values.\n",
    "  \n",
    "- **Ridge Regression:** Includes L2 regularization term to prevent overfitting, especially in the presence of multicollinearity.\n",
    "\n",
    "- **Lasso Regression:** Includes L1 regularization term, encouraging sparsity in the model and performing automatic feature selection.\n",
    "\n",
    "- **Elastic Net Regression:** Combines both L1 and L2 regularization terms, providing a balance between feature selection and coefficient shrinkage.\n",
    "\n",
    "- **Bayesian Linear Regression:** Applies a Bayesian approach, providing a probability distribution over the model parameters.\n",
    "\n",
    "- **Poisson Regression:** Used for predicting count data assuming a Poisson distribution for the response variable.\n",
    "\n",
    "- **Huber Regression:** Robust regression model less sensitive to outliers by using a combination of squared and absolute errors.\n",
    "\n",
    "- **Quantile Regression:** Models different quantiles of the response variable, providing insights into different parts of the distribution.\n",
    "\n",
    "- **Least Angle Regression (LARS):** Iteratively selects variables while keeping the coefficients as aligned as possible.\n",
    "\n",
    "- **Stepwise Regression:** Iteratively adds or removes variables based on their significance.\n",
    "\n",
    "- **Piecewise Regression:** Models different linear relationships for distinct ranges of the predictor variable.\n",
    "\n",
    "### **2. Linear Classification Models:**\n",
    "Linear classification models are used for predicting the class label of a data point. The general form of a linear classification equation is:\n",
    "\n",
    "\\[ P(Y=1) = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n)}} \\]\n",
    "\n",
    "- **Logistic Regression:** Models the probability of belonging to a particular class using the logistic function.\n",
    "\n",
    "- **Support Vector Machines (SVM):** Finds a hyperplane that best separates different classes, with variations for non-linear boundaries.\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA):** Models the distribution of input features for each class, assuming normality and equal covariance.\n",
    "\n",
    "- **Naive Bayes:** Applies Bayes' theorem with the \"naive\" assumption of feature independence given the class.\n",
    "\n",
    "- **Perceptron:** Simplest form of a neural network used for binary classification.\n",
    "\n",
    "- **Ridge and Lasso Logistic Regression:** Logistic regression models with L2 and L1 regularization, respectively.\n",
    "\n",
    "- **Elastic Net Logistic Regression:** Logistic regression model with a combination of L1 and L2 regularization terms.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD) Classifier:** Applies stochastic gradient descent optimization for training linear classifiers.\n",
    "\n",
    "These categories encompass a variety of linear models tailored for specific learning tasks and challenges. The choice of a particular linear model depends on factors such as the nature of the data, the task requirements, and the assumptions underlying the modeling approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
