{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Model Performance and its necessity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance refers to the ability of a machine learning model to make accurate predictions. It's typically measured using a performance metric that quantifies the difference between the model's predictions and the actual values. Common performance metrics include accuracy for classification problems, and mean squared error for regression problems.\n",
    "\n",
    "The necessity of evaluating model performance is crucial for several reasons:\n",
    "\n",
    "1. **Model Selection**: By evaluating the performance of different models or different configurations of a model, we can choose the one that performs best.\n",
    "\n",
    "2. **Overfitting Detection**: If a model performs well on the training data but poorly on the test data, it's likely overfitting to the training data. Evaluating model performance helps detect this.\n",
    "\n",
    "3. **Hyperparameter Tuning**: Model performance is used to tune the hyperparameters of a model. The model is trained with different hyperparameters, and the set of parameters that performs best on the validation set is chosen.\n",
    "\n",
    "4. **Trust in Model**: By knowing a model's performance, we can have an idea of how much we should trust its predictions. A model with high performance is likely to make accurate predictions, while a model with low performance might not be reliable.\n",
    "\n",
    "5. **Understanding Model's Strengths and Weaknesses**: Evaluating model performance can also help us understand where the model is doing well and where it's not. This can guide us in improving the model.\n",
    "\n",
    "In summary, evaluating model performance is a critical step in the machine learning process. It helps us choose the best model, tune its parameters, and understand its strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is K-Fold Cross Validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called 'k' that refers to the number of groups that a given data sample is to be split into.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. The dataset is shuffled and split into 'k' groups or folds. \n",
    "\n",
    "2. For each unique group:\n",
    "   - Take the group as a test data set.\n",
    "   - Take the remaining groups as a training data set.\n",
    "   - Fit a model on the training set and evaluate it on the test set.\n",
    "   - Retain the evaluation score and discard the model.\n",
    "\n",
    "3. The result of k-fold cross-validation is often given as the mean of the model skill scores. It is also good practice to include a measure of variance, such as the standard deviation or standard error.\n",
    "\n",
    "The choice of 'k' is usually 5 or 10, but there is no formal rule. As 'k' gets larger, the difference in size between the training set and the resampling subsets gets smaller, and the bias of the technique becomes smaller.\n",
    "\n",
    "K-Fold Cross Validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Life Cycle of K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The life cycle of K-Fold Cross-Validation involves several steps:\n",
    "\n",
    "1. **Shuffle and Split Data**: The entire dataset is first shuffled and then split into 'k' equal-sized subsets or folds.\n",
    "\n",
    "2. **Model Training and Evaluation**: For each unique group:\n",
    "   - One of the 'k' subsets is used as the validation set.\n",
    "   - The remaining 'k-1' subsets are combined and used as the training set.\n",
    "   - A model is trained on the training set.\n",
    "   - The trained model is then evaluated on the validation set. The evaluation score (e.g., accuracy, MSE, etc.) is retained.\n",
    "\n",
    "3. **Repeat Process**: Steps 2 is repeated 'k' times, each time with a different subset used as the validation set and the remaining subsets used as the training set.\n",
    "\n",
    "4. **Average Performance**: After 'k' iterations, we have 'k' evaluation scores. The final score used to evaluate the model's performance is typically the average of these 'k' scores. It's also common to compute the standard deviation or other measures of variability to get an idea of the uncertainty of the estimate.\n",
    "\n",
    "5. **Hyperparameter Tuning**: If the goal is to tune hyperparameters, the entire process is repeated for each combination of hyperparameters in the grid. The combination that gives the best average performance across the 'k' folds is chosen as the best hyperparameters.\n",
    "\n",
    "6. **Final Model Training**: Once we've chosen the best hyperparameters (if applicable), we train the final model on the entire dataset.\n",
    "\n",
    "The result of this process is a more robust estimate of model performance, by averaging over 'k' different train/test splits of the data. This helps to avoid overfitting to a particular train/test split and gives a better idea of how the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Python implementation of K-Fold Cross Validation using the `cross_val_score` function from the `sklearn.model_selection` module. This example uses the iris dataset from `sklearn.datasets` and a logistic regression model, but you can replace it with any dataset or model you like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "Average cross-validation score:  0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "scores = cross_val_score(model, iris.data, iris.target, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Average cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this code:\n",
    "\n",
    "- `cross_val_score()` performs K-Fold Cross Validation. The `cv` parameter specifies the number of folds.\n",
    "- `LogisticRegression()` is the model we're evaluating. You can replace this with any other model.\n",
    "- `iris` is the dataset we're using, a popular dataset for classification problems. You can replace this with your own data.\n",
    "- The cross-validation scores for each fold are printed, as well as the average score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of K-Fold Cross Validation in R using the `caret` package. This example uses the built-in `mtcars` dataset and a linear regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the caret package\n",
    "library(caret)\n",
    "\n",
    "# Define training control\n",
    "train_control <- trainControl(method = \"cv\", number = 5)\n",
    "\n",
    "# Train the model\n",
    "model <- train(mpg ~ ., data = mtcars, trControl = train_control, method = \"lm\")\n",
    "\n",
    "# Print out the results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this code:\n",
    "\n",
    "- `trainControl()` sets up the parameters for the cross-validation. The `method` parameter is set to `\"cv\"` for cross-validation, and the `number` parameter is set to 5 for 5-fold cross-validation.\n",
    "- `train()` trains the model. The formula `mpg ~ .` means \"predict `mpg` based on all other variables\". The `data` parameter is set to the `mtcars` dataset, `trControl` is set to the parameters defined earlier, and `method` is set to `\"lm\"` for linear regression.\n",
    "- The results are printed with `print()`. This includes the cross-validation results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
