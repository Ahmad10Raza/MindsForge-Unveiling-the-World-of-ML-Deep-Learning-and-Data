{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Cost Function (CF)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Cost Function, also known as a Loss Function or Objective Function, is a measure of how well a machine learning model is performing with respect to its given training sample and the expected output. It quantifies the error between predicted values and expected values and presents it in the form of a single real number. Depending on the problem, the cost function can be different.\n",
    "\n",
    "For example, in linear regression, the cost function is the Mean Squared Error (MSE) which calculates the average of the squared differences between the predicted and actual values. The goal of a machine learning model is to minimize this error during training.\n",
    "\n",
    "In classification problems like logistic regression, the cost function commonly used is Cross-Entropy or Log Loss. This function calculates the log of the likelihood that the model's predictions are correct for the given classes.\n",
    "\n",
    "The cost function is crucial in training machine learning models as it guides the optimization algorithms (like Gradient Descent) to adjust the model's parameters for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a real-life example related to a delivery company:\n",
    "\n",
    "A delivery company wants to minimize the total distance their delivery trucks travel in a day. The company operates in a large city with many different possible routes between any two locations. The company uses a machine learning algorithm to predict the most efficient routes for their deliveries each day.\n",
    "\n",
    "In this case, the cost function could be the total distance traveled by all delivery trucks in a day. The machine learning algorithm would use this cost function to evaluate the efficiency of a set of routes. If the total distance is less than the total distance for a different set of routes, the algorithm would consider it to be better.\n",
    "\n",
    "The algorithm's goal is to find the set of routes that minimizes the cost function - in other words, the set of routes that results in the least total distance traveled. By continually adjusting the routes based on the cost function, the algorithm can learn over time to predict more and more efficient routes.\n",
    "\n",
    "This is a simplification, of course. In reality, the cost function might also take into account other factors, like traffic, time of day, the number of deliveries each truck can make in a day, etc. But the basic idea is the same: the cost function measures how well the algorithm is doing, and the algorithm's goal is to minimize this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use Cost Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cost Function is a fundamental concept in machine learning and optimization. It's used for several reasons:\n",
    "\n",
    "1. **Measure of Prediction Error**: The cost function quantifies the error between the model's predictions and the actual data. A lower cost indicates better model performance.\n",
    "\n",
    "2. **Model Training**: During the training process, the goal is to find the model parameters that minimize the cost function. This is done using optimization algorithms like Gradient Descent.\n",
    "\n",
    "3. **Model Comparison**: Cost functions can be used to compare the performance of different models or different configurations of the same model. The model with the lowest cost is typically chosen as the best model.\n",
    "\n",
    "4. **Regularization**: Cost functions can also incorporate terms for regularization, which helps to prevent overfitting by penalizing complex models.\n",
    "\n",
    "5. **Guiding Model Improvement**: By analyzing how the cost function changes during training (i.e., learning curves), we can gain insights into how well the model is learning and whether adjustments need to be made (e.g., changing the learning rate, adding more data, etc.).\n",
    "\n",
    "In summary, cost functions are essential for training, evaluating, and improving machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Cost Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a categorization of cost functions based on the type of learning problem:\n",
    "\n",
    "**Regression Problems:**\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: It calculates the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**: It calculates the average of the absolute differences between the predicted and actual values. It's less sensitive to outliers than MSE.\n",
    "\n",
    "3. **Huber Loss**: This is often used in robust regression, it's less sensitive to outliers than MSE.\n",
    "\n",
    "**Classification Problems:**\n",
    "\n",
    "1. **Categorical Cross-Entropy Loss**: This measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "2. **Hinge Loss**: Used in Support Vector Machines, it's used for \"maximum-margin\" classification.\n",
    "\n",
    "3. **Binary Cross-Entropy**: This is a loss function used for binary classification problems.\n",
    "\n",
    "**Multi-class Classification Problems:**\n",
    "\n",
    "1. **Categorical Cross-Entropy**: This is a loss function used for multi-class classification problems.\n",
    "\n",
    "2. **Multi-class Hinge Loss**: A generalization of the hinge loss for more than two classes.\n",
    "\n",
    "**Other Learning Problems:**\n",
    "\n",
    "1. **Kullback-Leibler Divergence**: This is used in unsupervised learning, it measures how one probability distribution diverges from a second, expected probability distribution.\n",
    "\n",
    "2. **Negative Log Likelihood**: This is a loss function commonly used in the training of neural networks.\n",
    "\n",
    "Remember, the choice of cost function depends on the specific problem and data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
