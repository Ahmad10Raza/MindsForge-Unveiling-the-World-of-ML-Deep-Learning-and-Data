{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9402b7dc",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **Foundations of Speech Processing**\n",
    "\n",
    "* Basics of sound and speech signals\n",
    "* Digital audio representation: sampling rate, bit depth, spectrograms\n",
    "* Feature extraction techniques:\n",
    "\n",
    "  * **MFCCs (Mel-Frequency Cepstral Coefficients)**\n",
    "  * **Mel Spectrograms**\n",
    "  * **Log-Mel features**\n",
    "  * Chroma features\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Classical Speech Recognition Approaches**\n",
    "\n",
    "* **Acoustic Models** (mapping audio features to phonemes)\n",
    "* **Language Models** (predicting word sequences)\n",
    "* **Hidden Markov Models (HMMs)**\n",
    "* **Gaussian Mixture Models (GMMs)**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Deep Learning in Speech Recognition**\n",
    "\n",
    "* **RNNs (Recurrent Neural Networks)** for sequential data\n",
    "* **LSTMs/GRUs** for long context handling\n",
    "* **CTC (Connectionist Temporal Classification)** for aligning speech and text\n",
    "* **Seq2Seq Models with Attention**\n",
    "* **Transformers in Speech Recognition**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Modern Architectures**\n",
    "\n",
    "* **DeepSpeech (by Baidu)**\n",
    "* **Wav2Vec / Wav2Vec2.0 (Facebook/Meta)**\n",
    "* **Conformer models (Google)**\n",
    "* **Whisper (OpenAI)**\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Pre-trained Models & Frameworks**\n",
    "\n",
    "* Hugging Face Transformers for ASR (Wav2Vec2, Whisper, etc.)\n",
    "* OpenAI Whisper for multilingual speech-to-text\n",
    "* SpeechBrain, ESPnet, Kaldi, Fairseq\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Handling Real-World Challenges**\n",
    "\n",
    "* Noise reduction and speech enhancement\n",
    "* Speaker diarization (who spoke when)\n",
    "* Multilingual and code-switching speech\n",
    "* Domain adaptation (medical, legal, etc.)\n",
    "* Low-resource languages\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Practical Implementation**\n",
    "\n",
    "* Using **libraries**:\n",
    "\n",
    "  * `speech_recognition` (Python)\n",
    "  * Hugging Face `transformers` for Wav2Vec2, Whisper\n",
    "  * `torchaudio`\n",
    "  * Google Speech-to-Text API, Azure, AWS Transcribe\n",
    "* Streaming speech recognition (real-time STT)\n",
    "* Building an **end-to-end pipeline**:\n",
    "\n",
    "  * Audio input â†’ Feature extraction â†’ Model inference â†’ Text output\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Evaluation & Metrics**\n",
    "\n",
    "* Word Error Rate (WER)\n",
    "* Character Error Rate (CER)\n",
    "* Real-time Factor (RTF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b0554",
   "metadata": {},
   "source": [
    "### 1. **Foundations of Speech Processing**\n",
    "\n",
    "* Basics of sound and speech signals\n",
    "* Digital audio representation: sampling rate, bit depth, spectrograms\n",
    "* Feature extraction techniques:\n",
    "\n",
    "  * **MFCCs (Mel-Frequency Cepstral Coefficients)**\n",
    "  * **Mel Spectrograms**\n",
    "  * **Log-Mel features**\n",
    "  * Chroma features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254d5e8",
   "metadata": {},
   "source": [
    "\n",
    "### 1. What is Sound?\n",
    "\n",
    "* **Sound** is a vibration that propagates as a wave through a medium (like air).\n",
    "* It is a **mechanical wave**, not electromagnetic, meaning it needs a medium.\n",
    "* Represented as a **continuous analog signal** (waveform).\n",
    "\n",
    "Key properties of sound:\n",
    "\n",
    "* **Amplitude** â†’ Loudness (higher amplitude = louder sound).\n",
    "* **Frequency** (Hz) â†’ Pitch (higher frequency = higher pitch).\n",
    "* **Phase** â†’ Position of the wave at a given time.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is Speech?\n",
    "\n",
    "* **Speech** is a special type of sound signal produced by humans to convey language.\n",
    "* It is composed of **phonemes** (smallest units of sound in a language, e.g., /p/, /b/, /a/).\n",
    "* Speech has a structured pattern:\n",
    "\n",
    "  * **Prosody** (intonation, rhythm, stress).\n",
    "  * **Phonetics** (sounds).\n",
    "  * **Linguistics** (words, grammar, meaning).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Speech Signal Characteristics\n",
    "\n",
    "* **Non-stationary**: Speech varies over time (not constant like a pure sine wave).\n",
    "* **Quasi-periodic**: Contains repeating structures (like vowels), but not perfectly periodic.\n",
    "* **Time-domain representation**: The raw waveform as a function of time.\n",
    "* **Frequency-domain representation**: Breaks the signal into frequency components (via Fourier Transform).\n",
    "\n",
    "Example:\n",
    "\n",
    "* A **vowel** sound (like \"a\") is periodic, with clear frequency harmonics.\n",
    "* A **consonant** sound (like \"s\") is noisy and aperiodic.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Digital Representation of Speech\n",
    "\n",
    "Since computers canâ€™t process continuous analog signals directly, speech must be digitized.\n",
    "Steps:\n",
    "\n",
    "1. **Sampling** â†’ Converting continuous sound into discrete points (e.g., 16kHz = 16,000 samples/sec).\n",
    "2. **Quantization** â†’ Mapping amplitudes to numeric values (bit depth, e.g., 16-bit PCM).\n",
    "3. **Feature Extraction** â†’ Reducing raw data into meaningful features for models (like MFCCs, spectrograms).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Why Speech Processing is Hard?\n",
    "\n",
    "* **Noise**: Background sounds interfere with speech.\n",
    "* **Variability**: Different accents, speeds, and emotions.\n",
    "* **Co-articulation**: Sounds overlap when speaking naturally.\n",
    "* **Context**: Words depend on surrounding words.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In **Speech-to-Text systems**, the first step is to capture the sound signal and convert it into a digital representation that captures the essential features of speech while removing irrelevant noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674b71f",
   "metadata": {},
   "source": [
    "# Digital audio representation: sampling rate, bit depth, spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb2d85",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. **Sampling Rate**\n",
    "\n",
    "* **Definition**: The number of samples (measurements of amplitude) taken per second from a continuous audio signal to convert it into digital form.\n",
    "* **Measured in**: Hertz (Hz), or samples per second.\n",
    "* **Example**:\n",
    "\n",
    "  * 8,000 Hz â†’ Telephone quality (captures up to 4 kHz, sufficient for human speech intelligibility).\n",
    "  * 16,000 Hz (16 kHz) â†’ Common in speech recognition systems, balances quality and efficiency.\n",
    "  * 44,100 Hz (44.1 kHz) â†’ CD-quality audio, used for music.\n",
    "* **Concept**: According to the **Nyquist theorem**, to capture all information in a signal, the sampling rate should be at least twice the highest frequency present. For human speech (up to \\~8 kHz), 16 kHz is often enough.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Bit Depth**\n",
    "\n",
    "* **Definition**: The number of bits used to represent each audio sample (how precisely we record amplitude).\n",
    "* **Determines**: The **dynamic range** (difference between the quietest and loudest sounds).\n",
    "* **Examples**:\n",
    "\n",
    "  * 8-bit: Very low quality, noisy.\n",
    "  * 16-bit: CD quality, standard for most speech data.\n",
    "  * 24-bit or 32-bit: High precision, used in studios.\n",
    "* **Formula**: Dynamic range â‰ˆ 6.02 Ã— Bit Depth (in dB).\n",
    "\n",
    "  * 16-bit â†’ \\~96 dB dynamic range.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Spectrograms**\n",
    "\n",
    "* **Definition**: A 2D visual representation of sound that shows how frequencies in a signal change over time.\n",
    "* **Axes**:\n",
    "\n",
    "  * X-axis â†’ Time.\n",
    "  * Y-axis â†’ Frequency.\n",
    "  * Color/Intensity â†’ Amplitude (loudness of frequency components).\n",
    "* **Types**:\n",
    "\n",
    "  * **Linear Spectrogram** â†’ Frequencies shown linearly.\n",
    "  * **Log-Mel Spectrogram** â†’ Uses the **Mel scale**, which better matches human perception of pitch.\n",
    "* **Why important in speech-to-text**:\n",
    "  Speech recognition models (like DeepSpeech, Whisper, wav2vec) donâ€™t usually process raw waveforms directly. Instead, they work with **spectrograms or Mel-spectrograms**, which provide richer, structured representations.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary**\n",
    "\n",
    "* **Sampling Rate** defines how many slices per second of sound you capture.\n",
    "* **Bit Depth** defines how detailed each slice is.\n",
    "* **Spectrograms** transform raw audio into a timeâ€“frequency representation, making speech features easier for ML models to process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec9731",
   "metadata": {},
   "source": [
    "### **MFCCs (Mel-Frequency Cepstral Coefficients)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3600cbf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Why MFCCs are used\n",
    "\n",
    "* Raw audio signals are too detailed and noisy for speech recognition.\n",
    "* Human hearing is not linear â€” we are more sensitive to some frequencies than others.\n",
    "* MFCCs extract the most relevant features of speech (timbre, tone, phonetic structure) while reducing noise and redundancy.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The process of extracting MFCCs\n",
    "\n",
    "MFCC feature extraction involves multiple transformations:\n",
    "\n",
    "#### a) **Pre-emphasis**\n",
    "\n",
    "* Speech signal is passed through a filter to boost high frequencies.\n",
    "* This balances the energy between low and high frequencies.\n",
    "\n",
    "Equation:\n",
    "\n",
    "$$\n",
    "y(t) = x(t) - \\alpha \\cdot x(t-1)\n",
    "$$\n",
    "\n",
    "where $\\alpha \\approx 0.95$.\n",
    "\n",
    "---\n",
    "\n",
    "#### b) **Framing**\n",
    "\n",
    "* The audio signal is divided into short overlapping frames (e.g., 20â€“40 ms).\n",
    "* Speech is non-stationary, but in small frames, it can be considered stationary.\n",
    "\n",
    "---\n",
    "\n",
    "#### c) **Windowing**\n",
    "\n",
    "* Each frame is multiplied by a window function (like **Hamming window**) to reduce discontinuities at edges.\n",
    "\n",
    "---\n",
    "\n",
    "#### d) **Fast Fourier Transform (FFT)**\n",
    "\n",
    "* Converts the time-domain signal into frequency domain.\n",
    "* Produces the **power spectrum** of the signal.\n",
    "\n",
    "---\n",
    "\n",
    "#### e) **Mel Filter Bank**\n",
    "\n",
    "* Human ear perceives frequency on a **Mel scale** (logarithmic perception of pitch).\n",
    "* Apply triangular filters spaced along the Mel scale to emphasize frequencies important for speech.\n",
    "\n",
    "Mel scale formula:\n",
    "\n",
    "$$\n",
    "M(f) = 2595 \\cdot \\log_{10}\\left(1 + \\frac{f}{700}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### f) **Logarithm**\n",
    "\n",
    "* Logarithm is applied to the filter bank energies.\n",
    "* Mimics human perception of loudness (which is roughly logarithmic).\n",
    "\n",
    "---\n",
    "\n",
    "#### g) **Discrete Cosine Transform (DCT)**\n",
    "\n",
    "* DCT is applied to decorrelate features and compress information.\n",
    "* Result: A set of coefficients (usually 12â€“13 coefficients per frame) = **MFCCs**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. MFCC Representation\n",
    "\n",
    "* Each frame of speech is represented as a vector of MFCCs.\n",
    "* Adding **delta (Î”)** and **delta-delta (Î”Î”)** coefficients (time derivatives) captures speech dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why MFCCs are powerful\n",
    "\n",
    "* They reduce dimensionality while preserving phonetic information.\n",
    "* Used in **speech recognition, speaker identification, music classification, emotion detection**.\n",
    "* They mimic the **human auditory system** better than raw spectrum features.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In short: MFCCs take speech â†’ break into frames â†’ apply Fourier + Mel scaling + log + DCT â†’ produce compact numerical features that describe speech content effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b04f19",
   "metadata": {},
   "source": [
    "### *Mel Spectrograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fcac1",
   "metadata": {},
   "source": [
    "### Mel Spectrograms in Speech Processing\n",
    "\n",
    "A **mel spectrogram** is a way to represent audio signals, especially speech, in a format that aligns more closely with how humans perceive sound. It is widely used in **speech recognition, speaker identification, and speech synthesis**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Spectrogram Recap\n",
    "\n",
    "* A **spectrogram** shows how the frequency content of a signal changes over time.\n",
    "* It is generated by applying the **Short-Time Fourier Transform (STFT)** to split audio into small time frames, then analyzing frequency components.\n",
    "* Axes:\n",
    "\n",
    "  * X-axis â†’ time\n",
    "  * Y-axis â†’ frequency\n",
    "  * Color intensity â†’ amplitude (strength of frequency component)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Mel Scale\n",
    "\n",
    "* Human ears **do not perceive frequencies linearly**.\n",
    "\n",
    "  * Below 1 kHz â†’ humans are sensitive to small frequency changes.\n",
    "  * Above 1 kHz â†’ we perceive changes in a compressed, logarithmic way.\n",
    "* To mimic this, the **mel scale** is used:\n",
    "\n",
    "  * Formula:\n",
    "\n",
    "    $$\n",
    "    m = 2595 \\cdot \\log_{10}\\left(1 + \\frac{f}{700}\\right)\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "\n",
    "    * $m$ = frequency in mels\n",
    "    * $f$ = frequency in Hz\n",
    "\n",
    "This transformation compresses higher frequencies and expands lower ones to match human perception.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Mel Spectrogram Construction\n",
    "\n",
    "Steps to convert audio into a **mel spectrogram**:\n",
    "\n",
    "1. **Preprocessing**: Normalize audio and apply windowing.\n",
    "2. **STFT**: Break audio into short frames and compute Fourier transform.\n",
    "3. **Power Spectrum**: Convert magnitudes to power or log-power.\n",
    "4. **Apply Mel Filter Banks**:\n",
    "\n",
    "   * Filter banks are triangular filters spaced on the mel scale.\n",
    "   * They smooth out frequencies into perceptual bands.\n",
    "5. **Log Transformation**: Take log to approximate human loudness perception.\n",
    "\n",
    "Result â†’ a **2D image**:\n",
    "\n",
    "* Time (x-axis)\n",
    "* Mel frequency bins (y-axis)\n",
    "* Color intensity = log power\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why Use Mel Spectrograms?\n",
    "\n",
    "* Aligns with **human auditory perception** (important for speech tasks).\n",
    "* Reduces dimensionality compared to raw spectrograms.\n",
    "* Provides a **compact, robust representation** for machine learning models.\n",
    "* Used as input for **deep learning models** like CNNs and RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Applications\n",
    "\n",
    "* **Speech Recognition** (e.g., Google Speech-to-Text, Siri, Alexa).\n",
    "* **Speaker Recognition** (voice authentication).\n",
    "* **Emotion Detection from Speech**.\n",
    "* **Music Analysis** (genre classification, instrument detection).\n",
    "* **Text-to-Speech (TTS)** systems (mel spectrograms â†’ vocoder â†’ waveform).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
