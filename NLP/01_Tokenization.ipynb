{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c454b9e",
   "metadata": {},
   "source": [
    "# Tokenization & Text Preprocessing (NLP) ‚Äî Detailed Guide\n",
    "\n",
    "## 1) Why this matters\n",
    "\n",
    "Before any model can use text, you turn raw strings into **tokens** and (optionally) normalize them. Good preprocessing improves downstream accuracy, speed, and robustness; bad preprocessing throws away meaning (e.g., removing ‚Äúnot‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Tokenization: ways to split text\n",
    "\n",
    "### A) Sentence tokenization\n",
    "\n",
    "Split document ‚Üí sentences.\n",
    "\n",
    "* Use cases: summarization, translation, sentence-level classification.\n",
    "* Tools: `spaCy` (`Doc.sents`), `nltk.sent_tokenize`, `bltk`/IndicNLP for Indic langs.\n",
    "\n",
    "### B) Word tokenization (rule-based)\n",
    "\n",
    "Split sentences ‚Üí words with language rules (punctuation, clitics, contractions).\n",
    "\n",
    "* Pros: human-interpretable, fast\n",
    "* Cons: OOV issues, vocab explodes with morphology\n",
    "* Tools: `spaCy` (`Doc`), `nltk.word_tokenize`\n",
    "\n",
    "### C) Subword tokenization (modern, default for transformers)\n",
    "\n",
    "Break words into frequent pieces:\n",
    "\n",
    "* **BPE** (GPT/RoBERTa), **WordPiece** (BERT), **Unigram** (SentencePiece), **byte-level BPE** (GPT-2/3).\n",
    "* Pros: tiny OOV rate; handles misspellings/morphology; small vocab\n",
    "* Cons: less human-readable\n",
    "* Tools: `tokenizers` (HF), `sentencepiece`, model-specific fast tokenizers\n",
    "\n",
    "### D) Character/byte tokenization\n",
    "\n",
    "Every char/byte = token.\n",
    "\n",
    "* Pros: zero OOV, multilingual\n",
    "* Cons: long sequences, slower training; context modeling harder\n",
    "\n",
    "**Guideline:**\n",
    "\n",
    "* Classical ML (TF-IDF): word tokens (possibly lemma).\n",
    "* Transformers: subword tokenizer shipped with the model.\n",
    "* Morphologically rich or noisy text: subword or char-level.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Text preprocessing: what to do (and when)\n",
    "\n",
    "There‚Äôs no one-size-fits-all. Choose by task & model.\n",
    "\n",
    "### Core steps (often safe)\n",
    "\n",
    "1. **Unicode normalization** (`NFKC`) ‚Äì make accents/width forms consistent.\n",
    "2. **Whitespace normalization** ‚Äì collapse multiple spaces; keep sentence breaks if needed.\n",
    "3. **Lowercasing** ‚Äì only if your model is uncased (e.g., `bert-base-uncased`). Keep case for NER/sentiment unless uncased model.\n",
    "\n",
    "### Task-dependent steps\n",
    "\n",
    "* **Punctuation**:\n",
    "\n",
    "  * Keep for sentiment/emotion/NER; remove only for bag-of-words tasks where symbols add noise.\n",
    "* **Stopwords**:\n",
    "\n",
    "  * Remove for retrieval/TF-IDF; **keep** for transformers/sequence tasks (they carry syntax).\n",
    "* **Numbers**:\n",
    "\n",
    "  * Normalize to `<NUM>` for classical models to reduce sparsity; keep raw for transformers.\n",
    "* **URLs, mentions, hashtags**:\n",
    "\n",
    "  * Map to placeholders (`<URL>`, `<USER>`, `#Hashtag` ‚Üí `hashtag` + word) for classical models; transformers usually handle fine.\n",
    "* **Emojis/emoticons**:\n",
    "\n",
    "  * Convert to text (`üôÇ`‚Üí‚Äúsmiley\\_positive‚Äù) for sentiment; keep otherwise.\n",
    "* **Contractions & negation**:\n",
    "\n",
    "  * Expand (`don‚Äôt`‚Üí`do not`) to preserve negation for classical models; transformers OK either way‚Äî**never drop ‚Äúnot‚Äù**.\n",
    "* **Spelling correction**:\n",
    "\n",
    "  * Use carefully‚Äîcan distort named entities/usernames; helpful for ASR OCR noise.\n",
    "* **Stemming vs Lemmatization**:\n",
    "\n",
    "  * Stemming (Porter/Snowball) = fast, rough.\n",
    "  * Lemmatization (spaCy/WordNet) = slower, linguistically correct. Prefer lemmatization for classical features.\n",
    "\n",
    "### Language/domain specifics\n",
    "\n",
    "* Indic/Japanese/Chinese require specialized tokenizers (e.g., MeCab, jieba, IndicNLP).\n",
    "* Code-mixed text (Hindi+English): use multilingual models or custom rules.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Practical pipelines (code)\n",
    "\n",
    "### A) Classic ML pipeline (word tokens + lemmatize)\n",
    "\n",
    "```python\n",
    "import re, unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4'); nltk.download('stopwords')\n",
    "\n",
    "STOP = set(stopwords.words('english'))\n",
    "LEM = WordNetLemmatizer()\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_for_bow(text: str):\n",
    "    text = normalize(text).lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    # keep punctuation that signals negation; drop the rest if desired\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    kept = []\n",
    "    for t in tokens:\n",
    "        if t.isalpha() and t not in STOP:\n",
    "            kept.append(LEM.lemmatize(t))\n",
    "        elif t in {\"not\", \"no\"}:  # preserve negation\n",
    "            kept.append(t)\n",
    "    return kept\n",
    "\n",
    "print(preprocess_for_bow(\"I don't like this movie üòû. Visit https://ex.com\"))\n",
    "# ['not', 'like', 'movie']\n",
    "```\n",
    "\n",
    "### B) Transformers (use the model‚Äôs tokenizer; minimal cleaning)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"I don't like this movie üòû. Visit https://ex.com\"\n",
    "enc = tok(text, truncation=True, padding=\"max_length\", max_length=32, return_tensors=\"pt\")\n",
    "print(tok.convert_ids_to_tokens(enc[\"input_ids\"][0][:15]))\n",
    "```\n",
    "\n",
    "**Tip:** Avoid aggressive cleaning; the tokenizer knows how to handle URLs, emojis, casing (if uncased).\n",
    "\n",
    "### C) spaCy sentence + word tokens + lemmas\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple was founded by Steve Jobs. I don't like this movie.\")\n",
    "sents = [sent.text for sent in doc.sents]\n",
    "words = [(t.text, t.lemma_, t.pos_) for t in doc]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Evaluating preprocessing choices\n",
    "\n",
    "* **Sentiment**: keep emojis, punctuation like ‚Äú!‚Äù and negations; avoid stopword removal.\n",
    "* **NER**: keep casing, punctuation; no stemming; minimal cleaning.\n",
    "* **IR/keyword search**: lowercase, remove stopwords, lemmatize; maybe normalize numbers/URLs.\n",
    "* **Topic modeling**: lowercase, remove stopwords, lemmatize; keep nouns/adjectives.\n",
    "* **ASR/OCR noisy text**: consider light spell-correction and aggressive normalization.\n",
    "\n",
    "Always **A/B test**: train with/without a step and compare metrics (accuracy/F1/MAP).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Subword tokenizers in practice (quick notes)\n",
    "\n",
    "* **BPE**: merges frequent pairs; used by GPT/Roberta (often byte-level).\n",
    "* **WordPiece**: chooses pieces maximizing likelihood; used by BERT.\n",
    "* **Unigram (SentencePiece)**: probabilistic; used by XLM-R/T5.\n",
    "* **Byte-level BPE**: works on raw bytes ‚Üí robust to any script/emoji.\n",
    "\n",
    "**Implication:** Pre-tokenization is minimal (whitespace + punctuation split); **do not** alter input heavily.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Common pitfalls & fixes\n",
    "\n",
    "* **Over-cleaning** (removing ‚Äúnot‚Äù, emojis, punctuation) ‚Üí hurts sentiment/intent.\n",
    "* **Stopwords removal for transformers** ‚Üí unnecessary, harmful.\n",
    "* **Mixed casing with uncased models** ‚Üí fine; with cased models, don‚Äôt lowercase.\n",
    "* **Language mismatch** ‚Üí use correct language model/tokenizer.\n",
    "* **Leaking labels in preprocessing** ‚Üí never condition cleaning on labels.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Quick checklists\n",
    "\n",
    "**Classical ML (TF-IDF)**\n",
    "\n",
    "* [ ] Unicode normalize, lowercase\n",
    "* [ ] Replace URLs/users/numbers with placeholders\n",
    "* [ ] Tokenize words ‚Üí lemmatize\n",
    "* [ ] Remove stopwords (but keep negation)\n",
    "* [ ] Optional: bigrams/trigrams\n",
    "\n",
    "**Transformers**\n",
    "\n",
    "* [ ] Minimal cleaning (Unicode/whitespace)\n",
    "* [ ] Use the exact pretrained tokenizer\n",
    "* [ ] Truncate/pad to max length\n",
    "* [ ] Keep emojis, punctuation, URLs\n",
    "\n",
    "---\n",
    "\n",
    "If you tell me your **target task** (e.g., sentiment on Hindi-English tweets, resume parsing, Q\\&A, NER), I‚Äôll give you a tailored preprocessing recipe + starter code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2cc19",
   "metadata": {},
   "source": [
    "# Basic text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d5d36",
   "metadata": {},
   "source": [
    "Great question. Let‚Äôs go step by step on **basic text cleaning in NLP**, since it‚Äôs one of the most important preprocessing steps before applying tokenization, vectorization, or modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Text Cleaning is Needed?**\n",
    "\n",
    "Raw text data often comes with noise:\n",
    "\n",
    "* Extra spaces, punctuation, or numbers.\n",
    "* Case inconsistencies (e.g., \"Hello\" vs \"hello\").\n",
    "* Stopwords like *the, is, at* that may not add much meaning.\n",
    "* Special symbols, emojis, or HTML tags.\n",
    "\n",
    "If not cleaned, models may interpret these inconsistencies as different tokens, which hurts performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in Basic Text Cleaning**\n",
    "\n",
    "1. **Lowercasing**\n",
    "\n",
    "   * Convert all text into lowercase for consistency.\n",
    "   * Example: `\"Hello NLP World!\" ‚Üí \"hello nlp world!\"`.\n",
    "\n",
    "   ```python\n",
    "   text = \"Hello NLP World!\"\n",
    "   text = text.lower()\n",
    "   print(text)  # hello nlp world!\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "2. **Removing Punctuation**\n",
    "\n",
    "   * Punctuation usually does not add semantic meaning in most tasks.\n",
    "   * Example: `\"Hello, world!\" ‚Üí \"Hello world\"`\n",
    "\n",
    "   ```python\n",
    "   import string\n",
    "   text = \"Hello, NLP world!\"\n",
    "   cleaned = text.translate(str.maketrans('', '', string.punctuation))\n",
    "   print(cleaned)  # Hello NLP world\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "3. **Removing Extra Whitespace**\n",
    "\n",
    "   * Multiple spaces, tabs, or newlines are normalized to a single space.\n",
    "\n",
    "   ```python\n",
    "   text = \"This   is   NLP\\n\"\n",
    "   cleaned = \" \".join(text.split())\n",
    "   print(cleaned)  # This is NLP\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "4. **Removing Numbers (if not useful)**\n",
    "\n",
    "   * Sometimes digits don‚Äôt matter (e.g., product reviews).\n",
    "   * Example: `\"I bought 2 phones\"` ‚Üí `\"I bought phones\"`\n",
    "\n",
    "   ```python\n",
    "   import re\n",
    "   text = \"I bought 2 phones for 500 dollars\"\n",
    "   cleaned = re.sub(r'\\d+', '', text)\n",
    "   print(cleaned)  # I bought  phones for  dollars\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "5. **Removing Stopwords**\n",
    "\n",
    "   * Stopwords = very frequent words like *is, the, and*.\n",
    "   * Useful to reduce noise for bag-of-words or TF-IDF.\n",
    "   * Example: `\"This is an example\"` ‚Üí `\"example\"`\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk.corpus import stopwords\n",
    "   nltk.download('stopwords')\n",
    "\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "   text = \"This is an example of text cleaning\"\n",
    "   words = [w for w in text.split() if w.lower() not in stop_words]\n",
    "   print(words)  # ['example', 'text', 'cleaning']\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "6. **Removing Special Characters / HTML tags**\n",
    "\n",
    "   * Example: `\"Hello @user! <br> NLP is #awesome\"` ‚Üí `\"Hello NLP is awesome\"`.\n",
    "\n",
    "   ```python\n",
    "   text = \"Hello @user! <br> NLP is #awesome\"\n",
    "   cleaned = re.sub(r'<.*?>', '', text)   # remove HTML\n",
    "   cleaned = re.sub(r'[^a-zA-Z\\s]', '', cleaned)  # keep only letters\n",
    "   print(cleaned)  # Hello NLP is awesome\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "7. **Stemming or Lemmatization (Optional in Cleaning)**\n",
    "\n",
    "   * Stemming: Reduces words to root (e.g., \"playing\" ‚Üí \"play\").\n",
    "   * Lemmatization: Converts to dictionary form (e.g., \"better\" ‚Üí \"good\").\n",
    "   * Helps reduce vocabulary size.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Putting It Together**\n",
    "\n",
    "```python\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove extra whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "sample = \"Hello!!! I bought 2 Phones <br> and it is Amazing!!!\"\n",
    "print(clean_text(sample))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "hello bought phones amazing\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üìå So, **basic text cleaning in NLP = lowercasing + removing punctuation, numbers, stopwords, special symbols, and extra whitespace**.\n",
    "After this step, tokenization and feature extraction become much more reliable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c0e0e",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd70d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. What is Tokenization?\n",
    "\n",
    "* **Definition**: Tokenization is the process of breaking down a large piece of text (a sentence, paragraph, or document) into smaller units called **tokens**.\n",
    "* **Tokens** can be:\n",
    "\n",
    "  * Words ‚Üí e.g., `\"I love NLP\"` ‚Üí `[I, love, NLP]`\n",
    "  * Subwords ‚Üí e.g., `\"unhappiness\"` ‚Üí `[un, happiness]`\n",
    "  * Characters ‚Üí `\"cat\"` ‚Üí `[c, a, t]`\n",
    "  * Sentences ‚Üí `\"I love NLP. It is fun.\"` ‚Üí `[I love NLP, It is fun]`\n",
    "\n",
    "The main goal is to convert **raw text** into manageable chunks for further processing.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why is Tokenization Important in NLP?\n",
    "\n",
    "* Computers cannot directly understand raw text; they need **structured input**.\n",
    "* Tokenization is the **first step** before:\n",
    "\n",
    "  * Building vocabulary\n",
    "  * Converting words into numerical representations (word embeddings, one-hot encoding)\n",
    "  * Training models for tasks like classification, translation, sentiment analysis, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of Tokenization\n",
    "\n",
    "1. **Word Tokenization**\n",
    "\n",
    "   * Splits text into words.\n",
    "   * Example: `\"Natural Language Processing is cool!\"`\n",
    "     ‚Üí `[\"Natural\", \"Language\", \"Processing\", \"is\", \"cool\", \"!\"]`\n",
    "   * Tools: `nltk.word_tokenize()`, `spaCy`.\n",
    "\n",
    "2. **Sentence Tokenization**\n",
    "\n",
    "   * Splits text into sentences.\n",
    "   * Example: `\"I love NLP. It's amazing!\"`\n",
    "     ‚Üí `[\"I love NLP.\", \"It's amazing!\"]`\n",
    "   * Tools: `nltk.sent_tokenize()`.\n",
    "\n",
    "3. **Character Tokenization**\n",
    "\n",
    "   * Breaks text into individual characters.\n",
    "   * Example: `\"NLP\"` ‚Üí `[\"N\", \"L\", \"P\"]`\n",
    "   * Used in speech recognition, language modeling.\n",
    "\n",
    "4. **Subword Tokenization (Modern NLP models)**\n",
    "\n",
    "   * Breaks words into **sub-parts**.\n",
    "   * Example: `\"unhappiness\"` ‚Üí `[un, happiness]`\n",
    "   * Handles **out-of-vocabulary (OOV)** words better.\n",
    "   * Algorithms:\n",
    "\n",
    "     * **Byte Pair Encoding (BPE)** ‚Üí Used in GPT\n",
    "     * **WordPiece** ‚Üí Used in BERT\n",
    "     * **SentencePiece** ‚Üí Used in T5, XLNet\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tokenization Challenges\n",
    "\n",
    "* **Ambiguity**: `\"Let's eat, grandma\"` vs `\"Let's eat grandma\"`\n",
    "* **Punctuation**: Should `\"isn't\"` ‚Üí `[\"is\", \"n't\"]` or `[\"isn't\"]`?\n",
    "* **Languages**:\n",
    "\n",
    "  * English: spaces make tokenization easier.\n",
    "  * Chinese, Japanese: words are not separated by spaces, so tokenization is harder.\n",
    "* **Compound words**: `\"New York\"` should stay together.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Python Examples\n",
    "\n",
    "### Word Tokenization (NLTK)\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love learning NLP with Python!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "['I', 'love', 'learning', 'NLP', 'with', 'Python', '!']\n",
    "```\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I love NLP. It's fun and powerful.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "['I love NLP.', \"It's fun and powerful.\"]\n",
    "```\n",
    "\n",
    "### Tokenization with spaCy\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"FastAPI makes building APIs easy!\")\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "['FastAPI', 'makes', 'building', 'APIs', 'easy', '!']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "* Tokenization = breaking text into units (**words, sentences, characters, or subwords**).\n",
    "* It is the **foundation of NLP preprocessing**.\n",
    "* Modern NLP (transformers like BERT, GPT) prefer **subword tokenization**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47511cec",
   "metadata": {},
   "source": [
    "# normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21a36f",
   "metadata": {},
   "source": [
    "Normalization in NLP refers to transforming text into a consistent, standardized form so that different variations of words or text can be treated as the same. This is important because natural language is messy, and models or algorithms need uniform inputs to perform well.\n",
    "\n",
    "Here are the main techniques of normalization in NLP:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Case Normalization (Lowercasing/Uppercasing)**\n",
    "\n",
    "* Converts all text into the same case (usually lowercase).\n",
    "* Example:\n",
    "\n",
    "  * \"Apple\", \"APPLE\", \"apple\" ‚Üí \"apple\"\n",
    "\n",
    "Why? It avoids treating words as different just because of case.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Removing Punctuation and Special Characters**\n",
    "\n",
    "* Cleans symbols that are not meaningful for analysis (unless they are required, like in sentiment analysis with \"!\" or emojis).\n",
    "* Example:\n",
    "\n",
    "  * \"Hello!!! How are you???\" ‚Üí \"Hello How are you\"\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Stopword Removal**\n",
    "\n",
    "* Stopwords are common words that carry little meaning in tasks like classification.\n",
    "  Examples: *the, is, in, and, of, to, a*\n",
    "* Example:\n",
    "\n",
    "  * \"This is a good book\" ‚Üí \"good book\"\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Stemming**\n",
    "\n",
    "* Reduces a word to its **root form** by chopping suffixes.\n",
    "* It is rule-based and may produce non-dictionary words.\n",
    "* Example:\n",
    "\n",
    "  * \"playing\", \"played\", \"plays\" ‚Üí \"play\"\n",
    "  * \"studies\" ‚Üí \"studi\"\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Lemmatization**\n",
    "\n",
    "* Similar to stemming, but it uses a **dictionary (lexicon)** to return valid root words (lemmas).\n",
    "* It considers the **part of speech (POS)** to provide context-aware roots.\n",
    "* Example:\n",
    "\n",
    "  * \"better\" ‚Üí \"good\"\n",
    "  * \"studies\" ‚Üí \"study\"\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Handling Numbers**\n",
    "\n",
    "* Depending on the task, numbers can be removed, normalized, or replaced with a token.\n",
    "* Example:\n",
    "\n",
    "  * \"I have 2 apples\" ‚Üí \"I have NUM apples\"\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Unicode Normalization**\n",
    "\n",
    "* Converts characters into a consistent Unicode format.\n",
    "* Example:\n",
    "\n",
    "  * \"caf√©\" (with accent) ‚Üí \"cafe\"\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Expanding Contractions**\n",
    "\n",
    "* Converts contractions into their full form.\n",
    "* Example:\n",
    "\n",
    "  * \"don't\" ‚Üí \"do not\"\n",
    "  * \"I'm\" ‚Üí \"I am\"\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Handling Accents and Diacritics**\n",
    "\n",
    "* Removes or standardizes accents for consistency.\n",
    "* Example:\n",
    "\n",
    "  * \"r√©sum√©\" ‚Üí \"resume\"\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Normalization ensures that text variations like case, tenses, or suffixes don‚Äôt confuse NLP models.\n",
    "It often involves:\n",
    "\n",
    "* Lowercasing\n",
    "* Removing stopwords\n",
    "* Stemming/Lemmatization\n",
    "* Cleaning punctuation, numbers, and special symbols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558ec4b",
   "metadata": {},
   "source": [
    "**Thank You!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
