{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9188dd24",
   "metadata": {},
   "source": [
    "Pre-trained language models are one of the biggest breakthroughs in modern NLP. Instead of training models from scratch on small datasets, researchers train huge models on massive text corpora (billions of words) and then reuse (fine-tune) them for specific NLP tasks. This process is called **transfer learning in NLP**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What Are Pre-trained Language Models?\n",
    "\n",
    "* A **language model (LM)** is trained to predict the next word in a sentence or fill in missing words.\n",
    "* A **pre-trained language model** is one that has already been trained on a very large dataset (like Wikipedia, Common Crawl, or books).\n",
    "* Instead of starting from scratch, you can **fine-tune** the pre-trained model on your dataset for tasks like sentiment analysis, text classification, machine translation, or question answering.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Pre-trained Models?\n",
    "\n",
    "* **Save computation and time**: Training large models from scratch requires millions of GPU hours.\n",
    "* **Better performance**: They capture general language knowledge (semantics, grammar, context).\n",
    "* **Generalization**: They work well across different tasks, even with small labeled datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Examples of Pre-trained Models\n",
    "\n",
    "* **ELMo (Embeddings from Language Models)**\n",
    "\n",
    "  * Generates **contextualized word embeddings** (word meaning changes depending on context).\n",
    "  * Example: *bank* in “river bank” vs “financial bank.”\n",
    "\n",
    "* **GPT (Generative Pre-trained Transformer)**\n",
    "\n",
    "  * Autoregressive model trained to predict the next word.\n",
    "  * Good for text generation, summarization, dialogue.\n",
    "\n",
    "* **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "\n",
    "  * Trained with **masked language modeling** and **next sentence prediction**.\n",
    "  * Excels at classification, QA, sentiment analysis.\n",
    "\n",
    "* **RoBERTa, ALBERT, DistilBERT, XLNet**: Improvements over BERT.\n",
    "\n",
    "* **T5, BART**: Sequence-to-sequence models useful for summarization, translation, and generative tasks.\n",
    "\n",
    "* **GPT-3, GPT-4, LLaMA, Falcon**: Large-scale models for generative AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Usage of Pre-trained Language Models\n",
    "\n",
    "* **Feature Extraction**\n",
    "\n",
    "  * Use embeddings from hidden layers as features for downstream tasks.\n",
    "  * Example: Extract embeddings from BERT and use them in a classifier.\n",
    "\n",
    "* **Fine-tuning**\n",
    "\n",
    "  * Continue training the pre-trained model on your specific dataset.\n",
    "  * Example: Fine-tuning BERT for sentiment analysis with labeled movie reviews.\n",
    "\n",
    "* **Zero-shot / Few-shot Learning**\n",
    "\n",
    "  * Large models like GPT-3/4 can solve tasks without fine-tuning by just providing instructions or a few examples in the prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Advantages\n",
    "\n",
    "* Capture **contextual meaning** of words.\n",
    "* Reduce need for large labeled datasets.\n",
    "* Achieve **state-of-the-art performance** in many NLP benchmarks.\n",
    "\n",
    "### 6. Libraries and Tools\n",
    "\n",
    "* **Hugging Face Transformers** → the most popular library for using pre-trained models.\n",
    "\n",
    "  ```python\n",
    "  from transformers import pipeline\n",
    "\n",
    "  classifier = pipeline(\"sentiment-analysis\")\n",
    "  print(classifier(\"I love learning NLP!\"))\n",
    "  ```\n",
    "\n",
    "* **Spacy**, **AllenNLP**, **TensorFlow Hub**, **TorchText** also provide pre-trained models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7f10e",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f51e04",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. What is BERT?\n",
    "\n",
    "* **BERT** is a pre-trained language model developed by Google in 2018.\n",
    "* It is based on the **Transformer architecture** (specifically the encoder part).\n",
    "* Unlike previous models that read text left-to-right (or right-to-left), BERT reads **both directions simultaneously** (bidirectional).\n",
    "* This allows it to capture **contextual meaning** of words much better.\n",
    "\n",
    "Example:\n",
    "\n",
    "* In \"He went to the bank to withdraw money\" vs. \"He sat on the bank of the river,\" the word *bank* has different meanings.\n",
    "* BERT understands this difference because it considers **surrounding words (context)** in both directions.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Features\n",
    "\n",
    "* **Bidirectional attention**: Looks at entire sentence, not just left or right context.\n",
    "* **Pre-trained on massive corpora**: Trained on Wikipedia + BookCorpus using two tasks:\n",
    "\n",
    "  1. **Masked Language Modeling (MLM)**\n",
    "\n",
    "     * Randomly masks some words in the input and asks the model to predict them.\n",
    "     * Example: \"The cat sat on the \\[MASK]\" → Model predicts \"mat\".\n",
    "  2. **Next Sentence Prediction (NSP)**\n",
    "\n",
    "     * Determines if one sentence follows another logically.\n",
    "     * Example: \"I went to the store.\" → \"I bought milk.\" (Yes)\n",
    "       \"I went to the store.\" → \"The Earth is round.\" (No)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Architecture\n",
    "\n",
    "* Based on **Transformer Encoder**.\n",
    "* Stacked layers of self-attention + feedforward networks.\n",
    "* Versions:\n",
    "\n",
    "  * **BERT-Base**: 12 layers, 110M parameters.\n",
    "  * **BERT-Large**: 24 layers, 340M parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How BERT is Used\n",
    "\n",
    "* **Fine-tuning**:\n",
    "  After pre-training, BERT can be fine-tuned on a specific NLP task by adding a small output layer.\n",
    "  Examples:\n",
    "\n",
    "  * Sentiment analysis\n",
    "  * Named Entity Recognition (NER)\n",
    "  * Question answering (e.g., SQuAD dataset)\n",
    "  * Text classification\n",
    "\n",
    "* **Input format**:\n",
    "\n",
    "  * Sentences are tokenized into **WordPiece tokens**.\n",
    "  * Special tokens:\n",
    "\n",
    "    * `[CLS]` → Classification token at start.\n",
    "    * `[SEP]` → Separator token between sentences.\n",
    "  * Example input: `[CLS] I love NLP [SEP] It is amazing [SEP]`\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Advantages of BERT\n",
    "\n",
    "* Strong contextual understanding (better than Word2Vec, GloVe, FastText).\n",
    "* Achieved **state-of-the-art** results in many NLP benchmarks.\n",
    "* Can be fine-tuned for many downstream tasks with relatively small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Limitations\n",
    "\n",
    "* Computationally expensive (training and inference).\n",
    "* Requires large GPU/TPU resources.\n",
    "* Input length limitation (usually 512 tokens max).\n",
    "* Later models like **RoBERTa, DistilBERT, ALBERT, GPT, T5** improve on efficiency and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In summary:**\n",
    "BERT revolutionized NLP by introducing bidirectional contextual embeddings, enabling models to deeply understand meaning in text. It is still one of the most widely used foundations in modern NLP applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c05d3f",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504c261",
   "metadata": {},
   "source": [
    "### GPT Models in NLP\n",
    "\n",
    "**GPT (Generative Pre-trained Transformer)** is a family of language models developed by **OpenAI**. These models are based on the **Transformer architecture**, specifically the **decoder-only** part, and are designed for generating human-like text.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Architecture**\n",
    "\n",
    "* Built on the **Transformer Decoder** mechanism.\n",
    "* Uses **self-attention** layers to understand context.\n",
    "* Processes input sequentially (auto-regressive) and predicts the next word given previous words.\n",
    "* Unlike BERT (which uses bidirectional context), GPT looks **only left-to-right**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Training Process**\n",
    "\n",
    "1. **Pre-training**\n",
    "\n",
    "   * Trained on large amounts of unlabeled text (web data, books, articles).\n",
    "   * Objective: **Language modeling** → Predict the next token in a sequence.\n",
    "   * Example:\n",
    "\n",
    "     ```\n",
    "     Input: \"The dog is sitting on the\"\n",
    "     Model predicts: \"mat\"\n",
    "     ```\n",
    "\n",
    "2. **Fine-tuning** (in earlier versions like GPT-2)\n",
    "\n",
    "   * Adjusted on smaller, task-specific datasets (like sentiment analysis, QA).\n",
    "   * Later models (GPT-3, GPT-4) rely more on **prompt engineering** and few-shot learning instead of heavy fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Generations of GPT**\n",
    "\n",
    "* **GPT-1 (2018):**\n",
    "\n",
    "  * 117M parameters.\n",
    "  * First proof-of-concept that transformers + generative pre-training work.\n",
    "\n",
    "* **GPT-2 (2019):**\n",
    "\n",
    "  * 1.5B parameters.\n",
    "  * Famous for generating coherent long text, but initially withheld due to misuse concerns.\n",
    "\n",
    "* **GPT-3 (2020):**\n",
    "\n",
    "  * 175B parameters.\n",
    "  * Demonstrated **few-shot, one-shot, and zero-shot learning**.\n",
    "  * Powerful general-purpose model used widely in applications.\n",
    "\n",
    "* **GPT-4 (2023):**\n",
    "\n",
    "  * Multimodal (accepts text + images).\n",
    "  * More accurate, safer, and stronger reasoning abilities.\n",
    "  * Powers many real-world applications, including assistants like ChatGPT.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Key Features**\n",
    "\n",
    "* **Auto-regressive generation:** Predicts next token step by step.\n",
    "* **Context understanding:** Uses attention to capture long-range dependencies.\n",
    "* **Few-shot learning:** Learns tasks from just a few examples in the prompt.\n",
    "* **Scalability:** Larger models show emergent behaviors (better reasoning, creativity).\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Applications**\n",
    "\n",
    "* Text generation (stories, articles, dialogue).\n",
    "* Chatbots and virtual assistants.\n",
    "* Translation and summarization.\n",
    "* Code generation (e.g., GitHub Copilot).\n",
    "* Question answering.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Limitations**\n",
    "\n",
    "* **Bias & hallucination:** May generate incorrect or biased information.\n",
    "* **Lack of real understanding:** Predicts based on patterns, not true reasoning.\n",
    "* **Resource-intensive:** Requires massive compute and memory for training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "👉 In short, **GPT models revolutionized NLP** by showing that a single large pre-trained model can perform a wide variety of tasks with minimal fine-tuning, relying mostly on **prompts and few-shot learning**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ec1ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
