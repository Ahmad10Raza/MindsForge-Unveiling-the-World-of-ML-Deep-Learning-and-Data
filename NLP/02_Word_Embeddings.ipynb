{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa88ac2",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b6745",
   "metadata": {},
   "source": [
    "### Word Embeddings in NLP\n",
    "\n",
    "Word embeddings are **vector representations of words** that capture their meaning, context, and relationships. Unlike traditional representations like one-hot encoding (where each word is just a unique index), embeddings place words in a continuous vector space where **semantically similar words are closer together**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Why Word Embeddings?\n",
    "\n",
    "* **One-hot encoding limitations**:\n",
    "\n",
    "  * Creates very high-dimensional sparse vectors.\n",
    "  * No notion of similarity (e.g., \"king\" and \"queen\" are just different one-hot vectors).\n",
    "\n",
    "* **Embeddings solve this**:\n",
    "\n",
    "  * Represent words as **dense vectors** (e.g., 100–300 dimensions).\n",
    "  * Capture **semantic similarity** (e.g., vector(\"king\") is close to vector(\"queen\")).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Popular Word Embedding Techniques\n",
    "\n",
    "* **Word2Vec**\n",
    "\n",
    "  * Based on neural networks (CBOW and Skip-gram models).\n",
    "  * Learns word meaning from context.\n",
    "  * Famous example:\n",
    "\n",
    "    ```\n",
    "    vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")\n",
    "    ```\n",
    "\n",
    "* **GloVe (Global Vectors)**\n",
    "\n",
    "  * Learns embeddings by factorizing word co-occurrence matrices.\n",
    "  * Captures global statistical information.\n",
    "\n",
    "* **FastText**\n",
    "\n",
    "  * Extension of Word2Vec.\n",
    "  * Represents words as **subword (character n-grams)**.\n",
    "  * Handles **out-of-vocabulary (OOV)** words better.\n",
    "\n",
    "* **Contextual Embeddings (modern approaches)**\n",
    "\n",
    "  * Words get different embeddings depending on context.\n",
    "  * Examples: **ELMo, BERT, GPT**.\n",
    "  * \"Bank\" in *river bank* vs *money bank* will have different embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Properties of Word Embeddings\n",
    "\n",
    "* **Semantic similarity**: Words like \"car\" and \"automobile\" will have close vectors.\n",
    "* **Analogies**: Word relationships can be expressed as vector operations.\n",
    "* **Clustering**: Similar words form clusters (animals, colors, verbs, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Applications of Word Embeddings in NLP\n",
    "\n",
    "* Text classification (sentiment analysis, spam detection).\n",
    "* Machine translation.\n",
    "* Named entity recognition (NER).\n",
    "* Question answering and chatbots.\n",
    "* Semantic search and recommendation systems.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Example in Python (using Gensim Word2Vec)\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example corpus\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"machine\", \"learning\"],\n",
    "    [\"machine\", \"learning\", \"is\", \"awesome\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"machine\", \"learning\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "# Get embedding for a word\n",
    "print(\"Vector for 'machine':\\n\", model.wv['machine'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"Most similar to 'learning':\\n\", model.wv.most_similar('learning'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "✅ In short, **word embeddings transform words into meaningful numerical representations** that machine learning models can understand and use effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf08031",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf082a81",
   "metadata": {},
   "source": [
    "Alright, let’s dive into **Word2Vec** in detail. This is one of the most important foundational techniques in NLP for creating **word embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Word2Vec?\n",
    "\n",
    "* **Word2Vec** is a technique to represent words as **dense vectors** (embeddings) in a continuous vector space.\n",
    "* Developed by **Tomas Mikolov and team at Google (2013)**.\n",
    "* The idea: words with similar meanings should have similar vector representations.\n",
    "  Example:\n",
    "\n",
    "  * Vector(\"king\") – Vector(\"man\") + Vector(\"woman\") ≈ Vector(\"queen\")\n",
    "\n",
    "Instead of sparse **one-hot vectors**, Word2Vec produces **dense embeddings** where dimensions capture semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Main Architectures in Word2Vec\n",
    "\n",
    "Word2Vec uses shallow neural networks with two major approaches:\n",
    "\n",
    "1. **Continuous Bag of Words (CBOW)**\n",
    "\n",
    "   * Predicts the **target word** from surrounding context words.\n",
    "   * Example:\n",
    "     Sentence: \"The cat sits on the \\_\\_\\_\"\n",
    "     Context: \\[\"The\", \"cat\", \"sits\", \"on\", \"the\"] → predict \"mat\"\n",
    "   * Works well for small datasets and is faster.\n",
    "\n",
    "2. **Skip-Gram**\n",
    "\n",
    "   * Opposite of CBOW. It predicts the **context words** given the target word.\n",
    "   * Example:\n",
    "     Target word: \"mat\" → predict \\[\"The\", \"cat\", \"sits\", \"on\", \"the\"]\n",
    "   * Better for larger datasets and captures rare words well.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Word2Vec\n",
    "\n",
    "* Input layer: one-hot encoded word.\n",
    "* Hidden layer: produces embedding representation.\n",
    "* Output layer: predicts word probabilities using **softmax**.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "* Vocabulary size is huge → softmax is expensive.\n",
    "* Solutions:\n",
    "\n",
    "  * **Hierarchical Softmax**: speeds up computation using a binary tree.\n",
    "  * **Negative Sampling**: instead of updating weights for all words, update only for a small set of negative samples.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features of Word2Vec\n",
    "\n",
    "* Produces embeddings that capture **semantic relationships**.\n",
    "* Similar words cluster together (cosine similarity).\n",
    "* Can perform **vector arithmetic** on words.\n",
    "* Efficient to train even on large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Example in Python (using Gensim)\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sits\", \"on\", \"the\", \"mat\"],\n",
    "    [\"dogs\", \"play\", \"in\", \"the\", \"park\"],\n",
    "    [\"the\", \"boy\", \"plays\", \"football\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "# Get embedding for a word\n",
    "print(model.wv['cat'])\n",
    "\n",
    "# Find similar words\n",
    "print(model.wv.most_similar('cat'))\n",
    "```\n",
    "\n",
    "* `vector_size`: embedding dimension.\n",
    "* `window`: context window size.\n",
    "* `sg=1`: Skip-Gram; `sg=0`: CBOW.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of Word2Vec\n",
    "\n",
    "* Produces **static embeddings**: each word has one fixed vector.\n",
    "  Example: the word \"bank\" (river bank vs. financial bank) has the same embedding.\n",
    "* Struggles with **out-of-vocabulary (OOV)** words.\n",
    "* Later methods like **GloVe, FastText, and Transformers (BERT, GPT)** address these.\n",
    "\n",
    "---\n",
    "\n",
    "✅ In short: **Word2Vec is the foundation of modern NLP embeddings**. It transforms words into meaningful dense vectors, capturing semantic similarity and analogies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9fb079",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a48ee4",
   "metadata": {},
   "source": [
    "### GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "**GloVe** is a popular word embedding technique introduced by researchers at Stanford. It is similar in spirit to Word2Vec but is based on a different idea: instead of predicting a word based on context (like Word2Vec), GloVe directly uses **global statistical information** from the text corpus.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Core Idea**\n",
    "\n",
    "* Word2Vec is predictive: learns embeddings by predicting context words.\n",
    "* GloVe is count-based: builds a co-occurrence matrix of words and then factorizes it.\n",
    "\n",
    "The intuition is:\n",
    "\n",
    "* Words that appear in similar contexts should have similar vector representations.\n",
    "* For example: *“king – man + woman ≈ queen”* also emerges naturally in GloVe embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **How GloVe Works**\n",
    "\n",
    "1. **Co-occurrence Matrix**\n",
    "\n",
    "   * Build a matrix `X` where each entry `X_ij` = number of times word `j` appears in the context of word `i`.\n",
    "   * Example: in “I like playing football”, the co-occurrence of (“like”, “playing”) increases.\n",
    "\n",
    "2. **Probability Ratios**\n",
    "\n",
    "   * Instead of just raw counts, GloVe looks at **ratios of co-occurrence probabilities**.\n",
    "   * If two words `i` and `j` are related (say “ice” and “solid”), they should co-occur more often compared to unrelated words (say “ice” and “fashion”).\n",
    "\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   P_{ij} = \\frac{X_{ij}}{\\sum_k X_{ik}}\n",
    "   $$\n",
    "\n",
    "   where `P_ij` is the probability of seeing word `j` in the context of word `i`.\n",
    "\n",
    "3. **Training Objective**\n",
    "\n",
    "   * GloVe tries to find word vectors such that their **dot product approximates log of co-occurrence probability**.\n",
    "\n",
    "   $$\n",
    "   w_i^T \\cdot w_j + b_i + b_j \\approx \\log(X_{ij})\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   * `w_i` and `w_j` are word vectors\n",
    "   * `b_i`, `b_j` are bias terms\n",
    "\n",
    "   This converts co-occurrence statistics into useful embeddings.\n",
    "\n",
    "4. **Optimization**\n",
    "\n",
    "   * The loss function minimizes the difference between predicted values and `log(X_ij)`.\n",
    "   * A weighting function `f(X_ij)` is added so that very frequent words (like “the”, “is”) don’t dominate training.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Key Features of GloVe**\n",
    "\n",
    "* **Global + Local Context**: Combines the advantages of matrix factorization (global statistics) and neural embeddings (local context).\n",
    "* **Efficient**: Pre-trained GloVe embeddings are available (trained on billions of tokens like Wikipedia + Common Crawl).\n",
    "* **Good Semantic Properties**: Captures analogies, synonyms, and relationships well.\n",
    "\n",
    "Example:\n",
    "\n",
    "* *“Paris – France + Italy ≈ Rome”*\n",
    "* *“king – man + woman ≈ queen”*\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Pre-trained GloVe Embeddings**\n",
    "\n",
    "Commonly available pre-trained vectors:\n",
    "\n",
    "* 50d, 100d, 200d, 300d dimensions\n",
    "* Trained on different datasets like:\n",
    "\n",
    "  * Wikipedia 2014 + Gigaword 5\n",
    "  * Common Crawl (42B and 840B tokens)\n",
    "  * Twitter (27B tokens)\n",
    "\n",
    "You can download from [GloVe website](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Python Example (Using GloVe Pre-trained Vectors)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "# Example\n",
    "print(\"Vector for 'king':\")\n",
    "print(embeddings_index['king'])\n",
    "\n",
    "# Word analogy example: king - man + woman ≈ queen\n",
    "king = embeddings_index['king']\n",
    "man = embeddings_index['man']\n",
    "woman = embeddings_index['woman']\n",
    "\n",
    "vector = king - man + woman\n",
    "```\n",
    "\n",
    "You would then compare `vector` with all other embeddings to find the closest one (usually it will be “queen”).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Comparison with Word2Vec**\n",
    "\n",
    "* **Word2Vec**: Predictive, uses neural networks, focuses on local context windows.\n",
    "* **GloVe**: Count-based, factorizes co-occurrence matrix, uses global corpus statistics.\n",
    "\n",
    "Both methods produce embeddings with useful semantic relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47760a6e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **What is FastText?**\n",
    "FastText is a word embedding model developed by **Facebook AI Research (FAIR)**. It extends Word2Vec by representing words not just as atomic units, but as a collection of **character n-grams**.  \n",
    "This allows FastText to capture **subword information**, making it especially powerful for morphologically rich languages and handling out-of-vocabulary (OOV) words.\n",
    "\n",
    "---\n",
    "\n",
    "### **How it Works**\n",
    "1. **Word Representation**  \n",
    "   - Unlike Word2Vec, which treats each word as a single entity, FastText breaks a word into character-level **n-grams**.  \n",
    "   - Example:  \n",
    "     For the word `\"apple\"` with n-grams of size 3 (trigrams):  \n",
    "     - `<ap`, `app`, `ppl`, `ple`, `le>`  \n",
    "     - Special boundary symbols `< >` are used to distinguish prefixes/suffixes.\n",
    "\n",
    "2. **Vector Learning**  \n",
    "   - Each word’s embedding is the sum of its character n-gram embeddings.  \n",
    "   - Example: `\"apple\"` embedding = `sum(embedding(\"<ap\"), embedding(\"app\"), embedding(\"ppl\"), embedding(\"ple\"), embedding(\"le>\"))`.\n",
    "\n",
    "3. **Training Objective**  \n",
    "   - Similar to Word2Vec’s **skip-gram with negative sampling (SGNS)**.  \n",
    "   - Predict context words given the center word, but embeddings come from subwords.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of FastText**\n",
    "- **Handles Out-of-Vocabulary (OOV) Words**  \n",
    "  Since embeddings are built from character n-grams, FastText can generate embeddings for unseen words (unlike Word2Vec and GloVe).\n",
    "  - Example: `\"unhappiness\"` may not be in training, but embeddings can be built from `\"un\"`, `\"hap\"`, `\"ness\"`.\n",
    "  \n",
    "- **Better for Morphologically Rich Languages**  \n",
    "  Languages like German, Turkish, or Hindi, where words change forms (inflections), benefit a lot because FastText captures subword structure.  \n",
    "\n",
    "- **Captures Subword Semantics**  \n",
    "  Prefixes, suffixes, and roots contribute to meaning. `\"run\"`, `\"runner\"`, `\"running\"` will share overlapping n-grams and thus related embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages**\n",
    "- **Larger Model Size**  \n",
    "  Because it stores embeddings for many subword n-grams.  \n",
    "- **Slower Training**  \n",
    "  Compared to Word2Vec since more embeddings need to be learned.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example in Python (Using Gensim)**\n",
    "```python\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"fasttext\", \"is\", \"great\"],\n",
    "    [\"word\", \"embeddings\", \"are\", \"useful\"],\n",
    "    [\"nlp\", \"involves\", \"words\"]\n",
    "]\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences, vector_size=50, window=3, min_count=1, epochs=10)\n",
    "\n",
    "# Get word vector\n",
    "print(model.wv['fasttext'])\n",
    "\n",
    "# Get embedding for an OOV word\n",
    "print(model.wv['fasttexts'])  # Still works because it’s built from subwords\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Word2Vec and GloVe**\n",
    "| Feature                | Word2Vec        | GloVe           | FastText        |\n",
    "|------------------------|-----------------|-----------------|-----------------|\n",
    "| Uses co-occurrence?    | Local (context) | Global matrix   | Local (context) |\n",
    "| Subword info           | ❌              | ❌              | ✅              |\n",
    "| Handles OOV words      | ❌              | ❌              | ✅              |\n",
    "| Training speed         | Fast            | Medium          | Slower          |\n",
    "| Language suitability   | Works best for English | Works best for English | Works for morphologically rich languages |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a7bb3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. What are Pre-trained Word Embeddings?  \n",
    "Pre-trained word embeddings are **vector representations of words** that have been trained on a very large text corpus (like Wikipedia, Common Crawl, Google News, etc.) and are shared for reuse. Instead of training embeddings from scratch (which needs huge data and computation), we can directly use these pre-trained embeddings.  \n",
    "\n",
    "They capture **semantic meaning** of words, so words with similar meaning have vectors close to each other.  \n",
    "\n",
    "Examples:\n",
    "- Word2Vec (Google News dataset: 3 million words, 300 dimensions)  \n",
    "- GloVe (trained on Wikipedia + Gigaword, Common Crawl)  \n",
    "- FastText (trained on Common Crawl, supports subword embeddings)  \n",
    "- Transformer-based contextual embeddings: BERT, GPT embeddings  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Use Pre-trained Embeddings?  \n",
    "- **Save time and resources**: No need to train from scratch on billions of tokens.  \n",
    "- **Better performance**: Already trained on huge corpora, so embeddings are rich.  \n",
    "- **Transfer learning**: Knowledge from general text applies well to specific NLP tasks.  \n",
    "- **Handle rare words** (FastText, subword models).  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. How to Use Pre-trained Embeddings  \n",
    "There are **two main ways**:  \n",
    "\n",
    "#### A. Static Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "- Download pre-trained embeddings.  \n",
    "- Load them into your NLP pipeline.  \n",
    "- Replace words in your dataset with the corresponding vectors.  \n",
    "- Use embeddings as **features** in ML models (logistic regression, SVM, deep learning).  \n",
    "\n",
    "Example (GloVe with Gensim in Python):  \n",
    "```python\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Find vector for a word\n",
    "vector = glove_vectors['computer']\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = glove_vectors.most_similar('computer', topn=5)\n",
    "print(similar_words)\n",
    "```\n",
    "\n",
    "#### B. Contextual Embeddings (BERT, GPT, etc.)\n",
    "- Unlike Word2Vec/GloVe, BERT embeddings are **context-dependent**.  \n",
    "- Example: “bank” in *river bank* vs *bank account* → different vectors.  \n",
    "- Usually used via **transformer libraries** (like Hugging Face `transformers`).  \n",
    "\n",
    "Example (using BERT for embeddings):  \n",
    "```python\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text\n",
    "inputs = tokenizer(\"NLP is amazing\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings (last hidden state)\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings.shape)  # [batch_size, sequence_length, hidden_dim]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. When to Use Which  \n",
    "- **Word2Vec / GloVe / FastText**:  \n",
    "  Good for classical ML tasks or when deep contextual understanding is not required. Lightweight and fast.  \n",
    "\n",
    "- **BERT / Transformer embeddings**:  \n",
    "  Best for tasks like sentiment analysis, question answering, text classification, named entity recognition (NER). More accurate but computationally heavy.  \n",
    "\n",
    "---\n",
    "\n",
    "👉 In short: **Pre-trained embeddings give NLP systems a head start**, letting them leverage semantic knowledge from huge datasets instead of reinventing the wheel.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87dfac9",
   "metadata": {},
   "source": [
    "**Thank You!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
