{"cells":[{"cell_type":"markdown","metadata":{"id":"RrvS_JzwsrNk"},"source":["### Introduction\n","\n",">AlexNet was designed by Hinton, winner of the 2012 ImageNet competition, and his student Alex Krizhevsky. It was also after that year that more and deeper neural networks were proposed, such as the excellent vgg, GoogleLeNet. Its official data model has an accuracy rate of 57.1% and top 1-5 reaches 80.2%. This is already quite outstanding for traditional machine learning classification algorithms.\n","\n","\n","![title](https://raw.githubusercontent.com/entbappy/Branching-tutorial/19087e9920ff7db29e4103cc660bb41eca510b57/alexnet/alexnet.png)\n","\n","\n","![title](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/alexnet/alexnet2.png)\n","\n",">The following table below explains the network structure of AlexNet:\n","\n","\n","\n","<table>\n","<thead>\n","\t<tr>\n","\t\t<th>Size / Operation</th>\n","\t\t<th>Filter</th>\n","\t\t<th>Depth</th>\n","\t\t<th>Stride</th>\n","\t\t<th>Padding</th>\n","\t\t<th>Number of Parameters</th>\n","\t\t<th>Forward Computation</th>\n","\t</tr>\n","</thead>\n","<tbody>\n","\t<tr>\n","\t\t<td>3* 227 * 227</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Conv1 + Relu</td>\n","\t\t<td>11 * 11</td>\n","\t\t<td>96</td>\n","\t\t<td>4</td>\n","\t\t<td></td>\n","\t\t<td>(11*11*3 + 1) * 96=34944</td>\n","\t\t<td>(11*11*3 + 1) * 96 * 55 * 55=105705600</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>96 * 55 * 55</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Max Pooling</td>\n","\t\t<td>3 * 3</td>\n","\t\t<td></td>\n","\t\t<td>2</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>96 * 27 * 27</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Norm</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Conv2 + Relu</td>\n","\t\t<td>5 * 5</td>\n","\t\t<td>256</td>\n","\t\t<td>1</td>\n","\t\t<td>2</td>\n","\t\t<td>(5 * 5 * 96 + 1) * 256=614656</td>\n","\t\t<td>(5 * 5 * 96 + 1) * 256 * 27 * 27=448084224</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>256 * 27 * 27</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Max Pooling</td>\n","\t\t<td>3 * 3</td>\n","\t\t<td></td>\n","\t\t<td>2</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>256 * 13 * 13</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Norm</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Conv3 + Relu</td>\n","\t\t<td>3 * 3</td>\n","\t\t<td>384</td>\n","\t\t<td>1</td>\n","\t\t<td>1</td>\n","\t\t<td>(3 * 3 * 256 + 1) * 384=885120</td>\n","\t\t<td>(3 * 3 * 256 + 1) * 384 * 13 * 13=149585280</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>384 * 13 * 13</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Conv4 + Relu</td>\n","\t\t<td>3 * 3</td>\n","\t\t<td>384</td>\n","\t\t<td>1</td>\n","\t\t<td>1</td>\n","\t\t<td>(3 * 3 * 384 + 1) * 384=1327488</td>\n","\t\t<td>(3 * 3 * 384 + 1) * 384 * 13 * 13=224345472</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>384 * 13 * 13</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Conv5 + Relu</td>\n","\t\t<td>3 * 3</td>\n","\t\t<td>256</td>\n","\t\t<td>1</td>\n","\t\t<td>1</td>\n","\t\t<td>(3 * 3 * 384 + 1) * 256=884992</td>\n","\t\t<td>(3 * 3 * 384 + 1) * 256 * 13 * 13=149563648</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>256 * 13 * 13</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Max Pooling</td>\n","\t\t<td>3 * 3</td>\n","\t\t<td></td>\n","\t\t<td>2</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>256 * 6 * 6</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Dropout (rate 0.5)</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>FC6 + Relu</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n","\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>4096</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Dropout (rate 0.5)</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>FC7 + Relu</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td>4096 * 4096=16777216</td>\n","\t\t<td>4096 * 4096=16777216</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>4096</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>FC8 + Relu</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td>4096 * 1000=4096000</td>\n","\t\t<td>4096 * 1000=4096000</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>1000 classes</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Overall</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td>62369152=62.3 million</td>\n","\t\t<td>1135906176=1.1 billion</td>\n","\t</tr>\n","\t<tr>\n","\t\t<td>Conv VS FC</td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td></td>\n","\t\t<td>Conv:3.7million (6%) , FC: 58.6 million  (94% )</td>\n","\t\t<td>Conv: 1.08 billion (95%) , FC: 58.6 million (5%)</td>\n","\t</tr>\n","</tbody>\n","</table>\n","\n","\n","#### Why does AlexNet achieve better results?\n","\n","1. **Relu activation function is used.**\n","\n","Relu function: f (x) = max (0, x)\n","\n","![alex1](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/alexnet/alex512.png)\n","\n","ReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based networks. The following figure shows the number of iterations for a four-layer convolutional network based on CIFAR-10 that reached 25% training error in tanh and ReLU:\n","\n","![alex1](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/alexnet/alex612.png)\n","\n","2. **Standardization ( Local Response Normalization )**\n","\n","After using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the tanh and sigmoid functions, so a normalization will usually be done after ReLU, and the LRU is a steady proposal (Not sure here, it should be proposed?) One method in neuroscience is called \"Lateral inhibition\", which talks about the effect of active neurons on its surrounding neurons.\n","\n","![alex1](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/alexnet/alex3.jpg)\n","\n","\n","3. **Dropout**\n","\n","Dropout is also a concept often said, which can effectively prevent overfitting of neural networks. Compared to the general linear model, a regular method is used to prevent the model from overfitting. In the neural network, Dropout is implemented by modifying the structure of the neural network itself. For a certain layer of neurons, randomly delete some neurons with a defined probability, while keeping the individuals of the input layer and output layer neurons unchanged, and then update the parameters according to the learning method of the neural network. In the next iteration, rerandom Remove some neurons until the end of training.\n","\n","\n","![alex1](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/alexnet/alex4.jpg)\n","\n","\n","4. **Enhanced Data ( Data Augmentation )**\n","\n","\n","\n","**In deep learning, when the amount of data is not large enough, there are generally 4 solutions:**\n","\n",">  Data augmentation- artificially increase the size of the training set-create a batch of \"new\" data from existing data by means of translation, flipping, noise\n","\n",">  Regularization——The relatively small amount of data will cause the model to overfit, making the training error small and the test error particularly large. By adding a regular term after the Loss Function , the overfitting can be suppressed. The disadvantage is that a need is introduced Manually adjusted hyper-parameter.\n","\n",">  Dropout- also a regularization method. But different from the above, it is achieved by randomly setting the output of some neurons to zero\n","\n",">  Unsupervised Pre-training- use Auto-Encoder or RBM's convolution form to do unsupervised pre-training layer by layer, and finally add a classification layer to do supervised Fine-Tuning\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["AlexNet is a deep convolutional neural network architecture that played a crucial role in advancing the field of computer vision. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012.\n","\n","Here's an explanation of AlexNet along with an example code implementation using Python and TensorFlow/Keras:\n","\n","**Architecture of AlexNet:**\n","\n","AlexNet consists of eight layers, including five convolutional layers followed by three fully connected layers. It utilizes several key concepts that are now standard in modern convolutional neural networks, such as ReLU activation functions, overlapping pooling, and dropout.\n","\n","1. **Input Layer:** Accepts an input image of size (227, 227, 3) representing the RGB channels.\n","\n","2. **Convolutional Layers:** The first convolutional layer has 96 filters of size (11, 11, 3) with a stride of 4 and a ReLU activation function. The subsequent convolutional layers have 256, 384, 384, and 256 filters respectively, with smaller filter sizes and ReLU activations.\n","\n","3. **Max Pooling Layers:** After some of the convolutional layers, there are max pooling layers with a pool size of (3, 3) and a stride of 2.\n","\n","4. **Fully Connected Layers:** The convolutional layers are followed by three fully connected layers with 4096 neurons each. Dropout is applied to these layers to prevent overfitting.\n","\n","5. **Output Layer:** The final fully connected layer outputs probabilities for different classes. The output size depends on the number of classes in the classification task.\n","\n","**Example Code Implementation using TensorFlow/Keras:**\n","\n","```python\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","\n","# Define the AlexNet model\n","def create_alexnet(input_shape, num_classes):\n","    model = Sequential()\n","\n","    model.add(Conv2D(96, (11, 11), strides=(4, 4), input_shape=input_shape, activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","    \n","    model.add(Conv2D(256, (5, 5), padding='same', activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","    \n","    model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n","    \n","    model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n","    \n","    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","    \n","    model.add(Flatten())\n","    \n","    model.add(Dense(4096, activation='relu'))\n","    model.add(Dropout(0.5))\n","    \n","    model.add(Dense(4096, activation='relu'))\n","    model.add(Dropout(0.5))\n","    \n","    model.add(Dense(num_classes, activation='softmax'))\n","    \n","    return model\n","\n","# Define input shape and number of classes\n","input_shape = (227, 227, 3)\n","num_classes = 1000  # Assuming ImageNet dataset with 1000 classes\n","\n","# Create and compile the model\n","alexnet = create_alexnet(input_shape, num_classes)\n","alexnet.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","\n","# Print model summary\n","alexnet.summary()\n","```\n","\n","Note: The example code provides a simplified implementation of AlexNet using Keras. In practice, you would need to preprocess your data and provide a proper dataset for training and validation.\n","\n","Remember that deep learning frameworks and APIs may evolve over time, so the exact code syntax or functions used might differ if you are using a newer version of TensorFlow or Keras.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HTou3jNfuOuy"},"source":["# Code Implementation"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6008,"status":"ok","timestamp":1687488954988,"user":{"displayName":"Boktiar Ahmed Bappy","userId":"10381972055342951581"},"user_tz":-360},"id":"wW-lXn5VKjab","outputId":"f865b672-26c2-4096-9d05-433fa0547d74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tflearn\n","  Downloading tflearn-0.5.0.tar.gz (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (8.4.0)\n","Building wheels for collected packages: tflearn\n","  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127283 sha256=e7f49a1218d295ab407bb51ae50a3185f8f81ab1df8dcbc0ebd5599aac7c8c6c\n","  Stored in directory: /root/.cache/pip/wheels/55/fb/7b/e06204a0ceefa45443930b9a250cb5ebe31def0e4e8245a465\n","Successfully built tflearn\n","Installing collected packages: tflearn\n","Successfully installed tflearn-0.5.0\n"]}],"source":["!pip install tflearn"]},{"cell_type":"markdown","metadata":{},"source":["**TFLearn**, also known as Tensorflow Learn, was a high-level library built on top of TensorFlow, designed to make it easier to create and train neural networks. It provided a simplified interface for defining, training, and evaluating deep learning models.\n","\n","TFLearn aimed to simplify common deep learning tasks and provide a more intuitive API compared to working directly with TensorFlow. It offered features such as:\n","\n","1. **Easy Model Definition:** TFLearn allowed you to define neural network architectures using high-level abstractions, making it more concise and easier to understand.\n","\n","2. **Built-in Preprocessing:** It provided built-in data preprocessing and augmentation functions, making it convenient to preprocess your data before feeding it into the network.\n","\n","3. **Simple Training Loop:** TFLearn abstracted away some of the complexities of the training loop, making it easy to specify the optimizer, loss function, and metrics for training.\n","\n","4. **Visualization Tools:** TFLearn included tools for monitoring and visualizing training progress, such as real-time plots of loss and accuracy.\n","\n","However, as of my knowledge cutoff date in September 2021, TFLearn is no longer actively maintained, and its usage has significantly decreased in favor of using TensorFlow's native Keras API. TensorFlow's Keras provides a similar high-level interface for building and training neural networks, and it has become the recommended way to define and train models with TensorFlow.\n","\n","If you're looking to work with TensorFlow and want a high-level API, I recommend using TensorFlow's Keras API, which is well-documented, actively developed, and tightly integrated with TensorFlow. You can create, train, and evaluate deep learning models using Keras with TensorFlow as the backend."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3814,"status":"ok","timestamp":1687488958795,"user":{"displayName":"Boktiar Ahmed Bappy","userId":"10381972055342951581"},"user_tz":-360},"id":"1Q4PPu7WuUDN"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1153,"status":"ok","timestamp":1687489089951,"user":{"displayName":"Boktiar Ahmed Bappy","userId":"10381972055342951581"},"user_tz":-360},"id":"BxOVzfqXIImz"},"outputs":[],"source":["# Get Data\n","import tflearn.datasets.oxflower17 as oxflower17\n","from keras.utils import to_categorical\n","\n","x, y = oxflower17.load_data()\n","\n","x_train = x.astype('float32') / 255.0\n","y_train = to_categorical(y, num_classes=17)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1687489103800,"user":{"displayName":"Boktiar Ahmed Bappy","userId":"10381972055342951581"},"user_tz":-360},"id":"bHU9oT-vvwCt","outputId":"0a071b59-a0af-4b5b-e2bc-1fdcd1dc518a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1360, 224, 224, 3)\n","(1360, 17)\n"]}],"source":["print(x_train.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":653,"status":"ok","timestamp":1687488997591,"user":{"displayName":"Boktiar Ahmed Bappy","userId":"10381972055342951581"},"user_tz":-360},"id":"1QCMbyu5GhcE","outputId":"13c666b5-195c-400e-a0a7-a8a70aaad1ec"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n","                                                                 \n"," activation (Activation)     (None, 54, 54, 96)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n"," )                                                               \n","                                                                 \n"," batch_normalization (BatchN  (None, 26, 26, 96)       384       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 26, 26, 256)       614656    \n","                                                                 \n"," activation_1 (Activation)   (None, 26, 26, 256)       0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 12, 12, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 10, 10, 384)       885120    \n","                                                                 \n"," activation_2 (Activation)   (None, 10, 10, 384)       0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 10, 10, 384)      1536      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 8, 8, 384)         1327488   \n","                                                                 \n"," activation_3 (Activation)   (None, 8, 8, 384)         0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 8, 8, 384)        1536      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 6, 6, 256)         884992    \n","                                                                 \n"," activation_4 (Activation)   (None, 6, 6, 256)         0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 2, 2, 256)        1024      \n"," hNormalization)                                                 \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 4096)              4198400   \n","                                                                 \n"," activation_5 (Activation)   (None, 4096)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 4096)              0         \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 4096)             16384     \n"," hNormalization)                                                 \n","                                                                 \n"," dense_1 (Dense)             (None, 4096)              16781312  \n","                                                                 \n"," activation_6 (Activation)   (None, 4096)              0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 4096)              0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 4096)             16384     \n"," hNormalization)                                                 \n","                                                                 \n"," dense_2 (Dense)             (None, 17)                69649     \n","                                                                 \n"," activation_7 (Activation)   (None, 17)                0         \n","                                                                 \n","=================================================================\n","Total params: 24,834,833\n","Trainable params: 24,815,697\n","Non-trainable params: 19,136\n","_________________________________________________________________\n"]}],"source":["# Create a sequential model\n","model = Sequential()\n","\n","# 1st Convolutional Layer\n","model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n","model.add(Activation('relu'))\n","\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n","# Batch Normalisation before passing it to the next layer\n","model.add(BatchNormalization())\n","\n","# 2nd Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n","model.add(Activation('relu'))\n","\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","\n","# 3rd Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 4th Convolutional Layer\n","model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","# 5th Convolutional Layer\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n","model.add(Activation('relu'))\n","\n","\n","# Pooling\n","model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","\n","# Passing it to a dense layer\n","model.add(Flatten())\n","\n","# 1st Dense Layer\n","model.add(Dense(4096, input_shape=(224*224*3,)))\n","model.add(Activation('relu'))\n","# Add Dropout to prevent overfitting\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# 2nd Dense Layer\n","model.add(Dense(4096))\n","model.add(Activation('relu'))\n","# Add Dropout\n","model.add(Dropout(0.4))\n","# Batch Normalisation\n","model.add(BatchNormalization())\n","\n","# Output Layer\n","model.add(Dense(17))\n","model.add(Activation('softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":697,"status":"ok","timestamp":1687489117787,"user":{"displayName":"Boktiar Ahmed Bappy","userId":"10381972055342951581"},"user_tz":-360},"id":"MMSou5UCGkQs"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22647,"status":"ok","timestamp":1687489147500,"user":{"displayName":"Boktiar Ahmed Bappy","userId":"10381972055342951581"},"user_tz":-360},"id":"S8Brj24fGpM9","outputId":"b10fca51-d25b-4068-ddcc-13709c39caab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 1088 samples, validate on 272 samples\n","Epoch 1/5\n","1088/1088 [==============================] - ETA: 0s - loss: 3.6493 - acc: 0.2858"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1088/1088 [==============================] - 11s 10ms/sample - loss: 3.6493 - acc: 0.2858 - val_loss: 3.1504 - val_acc: 0.0699\n","Epoch 2/5\n","1088/1088 [==============================] - 2s 2ms/sample - loss: 2.0367 - acc: 0.4357 - val_loss: 5.0040 - val_acc: 0.0699\n","Epoch 3/5\n","1088/1088 [==============================] - 2s 2ms/sample - loss: 1.7029 - acc: 0.5055 - val_loss: 7.8723 - val_acc: 0.0551\n","Epoch 4/5\n","1088/1088 [==============================] - 2s 2ms/sample - loss: 1.5586 - acc: 0.5441 - val_loss: 5.6248 - val_acc: 0.0699\n","Epoch 5/5\n","1088/1088 [==============================] - 2s 2ms/sample - loss: 1.3488 - acc: 0.6094 - val_loss: 7.2296 - val_acc: 0.0699\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fe7577e01f0>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Train\n","model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1,validation_split=0.2, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHaBso7TG6YU"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMksMq5pGv6NtyECDmLbejo","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
