
## ğŸ§­ Roadmap to Fine-Tune LayoutLM on PO Documents

---

### ğŸ“Œ **Phase 1: Preparation & Setup**

#### âœ… 1. Environment Setup

* Install necessary tools:

```bash
pip install transformers datasets seqeval pytorch torchvision
pip install -U layoutparser
```

#### âœ… 2. Choose LayoutLM Version

* Use `LayoutLMv1` for text + layout.
* Use `LayoutLMv2` or `LayoutLMv3` for text + layout + image features.

#### âœ… 3. OCR Tool Selection

* Use OCR tools like:
  * [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)
  * [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)
  * [Azure Read OCR API]

---

### ğŸ“Œ **Phase 2: Data Preparation**

#### âœ… 4. Prepare Annotated Dataset

You need annotated PO documents with:

* OCR-extracted `text`
* Bounding boxes `[x0, y0, x1, y1]`
* Corresponding labels (`B-PO_NUMBER`, `I-PO_NUMBER`, etc.)

Format: FUNSD-style or SROIE-style

```json
{
  "id": "po_123",
  "words": ["PO", "Number", ":", "123456"],
  "bboxes": [[100, 50, 150, 70], [160, 50, 230, 70], [240, 50, 250, 70], [260, 50, 310, 70]],
  "labels": ["O", "O", "O", "B-PO_NUMBER"]
}
```

#### âœ… 5. Use Label Studio (optional)

* Upload scanned PDFs/images.
* Draw bounding boxes and assign labels.
* Export annotations in COCO or JSON format.

---

### ğŸ“Œ **Phase 3: Model & Tokenizer Setup**

#### âœ… 6. Load Pretrained LayoutLM

```python
from transformers import LayoutLMForTokenClassification, LayoutLMTokenizer

tokenizer = LayoutLMTokenizer.from_pretrained("microsoft/layoutlm-base-uncased")
model = LayoutLMForTokenClassification.from_pretrained("microsoft/layoutlm-base-uncased", num_labels=NUM_CLASSES)
```

#### âœ… 7. Encode Dataset

Convert texts, labels, and bounding boxes to model input format using tokenizer.

---

### ğŸ“Œ **Phase 4: Fine-Tuning**

#### âœ… 8. Training Loop Setup (Hugging Face Trainer)

Use `Trainer` from Hugging Face:

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./layoutlm-po",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    evaluation_strategy="steps",
    save_steps=500,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics_fn,
)

trainer.train()
```

#### âœ… 9. Evaluate Model

Use `seqeval` to evaluate named entity metrics:

* Precision
* Recall
* F1-score

---

### ğŸ“Œ **Phase 5: Inference Pipeline**

#### âœ… 10. Build Inference Script

Take a new PO document, run OCR â†’ tokenize â†’ predict â†’ format JSON:

```python
outputs = model(**inputs)
predictions = torch.argmax(outputs.logits, dim=-1)
```

#### âœ… 11. Output Example

```json
{
  "PO_NUMBER": "123456",
  "VENDOR": "ABC Pvt Ltd",
  "DATE": "2025-06-21"
}
```

---

### ğŸ“Œ **Phase 6: Optimization & Deployment**

#### âœ… 12. Improve Accuracy

* Use `LayoutLMv2` with visual embeddings for noisy PDFs.
* Fine-tune on more labeled POs (diverse vendors).
* Use domain-specific vocab (custom tokenizer).

#### âœ… 13. Integrate into Automation Anywhere

* Wrap inference as a REST API (Flask/FastAPI).
* Automation Anywhere can send base64 image and receive JSON.

---

## ğŸ Final Tools Checklist

| Component  | Tool                               |
| ---------- | ---------------------------------- |
| OCR        | Tesseract / PaddleOCR / Azure Read |
| Labeling   | Label Studio                       |
| Model      | LayoutLMv1/v2/v3                   |
| Training   | Hugging Face Transformers          |
| Evaluation | SeqEval                            |
| Deployment | Flask / FastAPI + AA API call      |
