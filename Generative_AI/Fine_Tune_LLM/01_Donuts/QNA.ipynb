{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e74e69b",
   "metadata": {},
   "source": [
    "# what about other model like ollama for information extraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329edeab",
   "metadata": {},
   "source": [
    "## üÜö Donut vs Ollama (LLaMA/Mistral)\n",
    "\n",
    "| Feature               | **Donut**                                         | **Ollama Models (Mistral, LLaMA, etc.)**      |\n",
    "| --------------------- | ------------------------------------------------- | --------------------------------------------- |\n",
    "| Input Type            | Document **images** (OCR-free)                    | **Text only**                                 |\n",
    "| Layout-aware          | ‚úÖ Yes (learns layout visually)                    | ‚ùå No (text only, no spatial/layout info)      |\n",
    "| Handles scanned PDFs  | ‚úÖ Yes (via images)                                | ‚ùå No (needs OCR first)                        |\n",
    "| Requires OCR          | ‚ùå No                                              | ‚úÖ Yes (Tesseract, Textract, etc.)             |\n",
    "| Fine-tuning           | ‚úÖ Supported                                       | ‚úÖ Supported via LoRA/QLoRA                    |\n",
    "| Output Format         | JSON via sequence generation                      | Prompt-based JSON/text generation             |\n",
    "| Structured extraction | ‚úÖ High accuracy with enough data                  | ‚ùå Less reliable if layout is complex          |\n",
    "| Deployment            | Slightly heavier (image model)                    | Lightweight, easy via Ollama CLI              |\n",
    "| Best for              | Forms, invoices, receipts, layout-based documents | Chat-style document Q\\&A, summarization, etc. |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ When to Use **Donut**\n",
    "\n",
    "Use **Donut** when:\n",
    "\n",
    "* You need **structured extraction** (e.g., invoice number, total, date).\n",
    "* Your documents are **scanned** or **layout matters**.\n",
    "* You want to **avoid OCR** and handle messy formats natively.\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ When to Use **Ollama (LLaMA/Mistral etc.)**\n",
    "\n",
    "Use **Ollama** when:\n",
    "\n",
    "* You can **OCR the invoice first** to get clean text.\n",
    "* You want a **lightweight local LLM** for prompt-based extraction.\n",
    "* You don't need layout-level precision.\n",
    "* You want a **general-purpose assistant** that can summarize, extract info, etc.\n",
    "\n",
    "### üì¶ Example Ollama Use Case\n",
    "\n",
    "1. OCR the invoice to text using Tesseract or any API.\n",
    "2. Use a prompt like:\n",
    "\n",
    "   ```\n",
    "   Extract these fields from the invoice:\n",
    "   - Invoice Number\n",
    "   - Vendor Name\n",
    "   - Amount\n",
    "   - Date\n",
    "\n",
    "   Text: [OCR_RESULT]\n",
    "   ```\n",
    "3. Run it via:\n",
    "\n",
    "   ```bash\n",
    "   ollama run mistral\n",
    "   ```\n",
    "\n",
    "### üõ† Automate via Python:\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "def query_ollama(prompt):\n",
    "    cmd = ['ollama', 'run', 'mistral']\n",
    "    proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    out, _ = proc.communicate(prompt.encode())\n",
    "    return out.decode()\n",
    "\n",
    "text = extract_text_from_pdf(\"invoice.pdf\")  # via OCR\n",
    "prompt = f\"\"\"Extract Invoice Number, Date, Amount, Vendor from:\\n{text}\"\"\"\n",
    "print(query_ollama(prompt))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Final Recommendation\n",
    "\n",
    "| Scenario                              | Suggested Model         |\n",
    "| ------------------------------------- | ----------------------- |\n",
    "| Scanned + text invoices, layout-based | ‚úÖ Donut                 |\n",
    "| All OCRed text, no layout needed      | ‚úÖ Ollama (Mistral etc.) |\n",
    "| Prompt-based, quick prototyping       | ‚úÖ Ollama                |\n",
    "| High-accuracy JSON output             | ‚úÖ Donut                 |\n",
    "\n",
    "\n",
    "\n",
    "If you have:\n",
    "\n",
    "* **Good OCR + consistent data:** Use Ollama.\n",
    "* **Mixed, messy invoices + layout-dependency:** Stick with Donut.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536fd71",
   "metadata": {},
   "source": [
    "# the model `donut-base-finetuned-synthdog-naver` trained on korean data so there is any conflict between english and corean beacuse i have english datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd719a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Yes, the model `donut-base-finetuned-synthdog-naver` **was trained on Korean-style synthetic invoice data**, but **here‚Äôs the full breakdown of compatibility and recommendations** for your **English invoice dataset**:\n",
    "\n",
    "\n",
    "\n",
    "## üåê Language Compatibility of `donut-base-finetuned-synthdog-naver`\n",
    "\n",
    "| Aspect                            | Status                                                                 |\n",
    "| --------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **OCR-free text extraction**      | ‚úÖ Language-agnostic (image-to-sequence)                                |\n",
    "| **Pretraining base model**        | ‚úÖ Trained on multilingual documents, including English                 |\n",
    "| **Fine-tuning target data**       | üî∂ Mostly Korean-style data (structure similar, language is different) |\n",
    "| **English invoice compatibility** | üî∂ Works *to some extent*, but fine-tuning is **strongly recommended** |\n",
    "\n",
    "\n",
    "\n",
    "## ‚ö†Ô∏è Potential Issues You Might Face\n",
    "\n",
    "1. **Text Decoding Bias**: The model may generate Korean tokens or formatting styles if not fine-tuned.\n",
    "2. **Field Confusion**: It might misinterpret English date/currency formats.\n",
    "3. **Noise in Output**: Occasional Korean labels, template tags, or encoding artifacts.\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ Recommended Approach for English Invoices\n",
    "\n",
    "### ‚úÖ Best Practice:\n",
    "\n",
    "> **Use `donut-base` as the starting point and fine-tune it on your English invoices.**\n",
    "\n",
    "Why?\n",
    "\n",
    "* It's **clean**, **language-agnostic**, and doesn't carry language-specific biases.\n",
    "* You have **full control** over your output format (e.g., JSON).\n",
    "\n",
    "\n",
    "\n",
    "## üß† Alternative (if you want a pre-finetuned starting point):\n",
    "\n",
    "Use `donut-base-finetuned-docvqa` (for English QA) or CORD if fields are similar, **but always fine-tune** it with your English structured data.\n",
    "\n",
    "\n",
    "\n",
    "## üîÑ Summary:\n",
    "\n",
    "| Option                                | Use for English Invoices?  | Notes                                                    |\n",
    "| ------------------------------------- | -------------------------- | -------------------------------------------------------- |\n",
    "| `donut-base`                          | ‚úÖ Best choice              | Neutral base, fine-tune with your data                   |\n",
    "| `donut-base-finetuned-synthdog-naver` | üî∂ Possible, but not ideal | Fine-tuned on Korean. May output Korean tags/format      |\n",
    "| `donut-base-finetuned-cord-v2`        | üî∂ Use with caution        | Korean receipts, English not guaranteed                  |\n",
    "| `donut-base-finetuned-docvqa`         | ‚úÖ For QA-based extraction  | Only for Q\\&A, not full structured JSON field extraction |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5dd35",
   "metadata": {},
   "source": [
    "# All i want to train model on my english invoice then receive output in json format so what is best option?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e475d0",
   "metadata": {},
   "source": [
    "‚úÖ Based on your requirement:\n",
    "\n",
    "> **Train a model on English invoice documents and get structured JSON output** (e.g., invoice number, date, vendor, amount)\n",
    "\n",
    "\n",
    "\n",
    "## üèÜ **Best Option: `donut-base`**\n",
    "\n",
    "### Why?\n",
    "\n",
    "| Reason                                | Explanation                                                       |\n",
    "| ------------------------------------- | ----------------------------------------------------------------- |\n",
    "| ‚úÖ Clean and language-neutral          | Not biased toward Korean (unlike `synthdog` or `CORD` variants)   |\n",
    "| ‚úÖ Designed for image ‚Üí JSON parsing   | You can directly define your custom fields in the output          |\n",
    "| ‚úÖ Ideal for fine-tuning               | Works best when trained on your own dataset                       |\n",
    "| ‚úÖ Structured field extraction         | Generates JSON-style outputs from document images (no OCR needed) |\n",
    "| ‚úÖ Officially supported by HuggingFace | Easy integration, training, and deployment                        |\n",
    "\n",
    "\n",
    "\n",
    "## üéØ Output Example (after fine-tuning)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"invoice_number\": \"INV-2024-0981\",\n",
    "  \"invoice_date\": \"2024-05-31\",\n",
    "  \"vendor\": \"ABC Pvt Ltd\",\n",
    "  \"amount\": \"$1,500.00\",\n",
    "  \"payment_due\": \"2024-06-15\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## üõ†Ô∏è What You Need to Do\n",
    "\n",
    "### 1. Use this base model:\n",
    "\n",
    "```bash\n",
    "naver-clova-ix/donut-base\n",
    "```\n",
    "\n",
    "### 2. Prepare data like:\n",
    "\n",
    "* Convert invoice PDFs to images (JPEG/PNG)\n",
    "* Label your ground-truth in a consistent JSON format\n",
    "* Flatten JSON to a tag-based string format for Donut like:\n",
    "\n",
    "  ```text\n",
    "  <s_invoice><invoice_number>INV-2024-0981</invoice_number><invoice_date>2024-05-31</invoice_date>...</s_invoice>\n",
    "  ```\n",
    "\n",
    "### 3. Fine-tune `donut-base` using HuggingFace Transformers\n",
    "\n",
    "\n",
    "\n",
    "## üìå Summary:\n",
    "\n",
    "| Goal                             | Best Model   | Reason                          |\n",
    "| -------------------------------- | ------------ | ------------------------------- |\n",
    "| Train on English invoices        | `donut-base` | Language-neutral + customizable |\n",
    "| Output structured JSON           | `donut-base` | Natively supports JSON output   |\n",
    "| Avoid OCR                        | `donut-base` | Image-to-sequence transformer   |\n",
    "| Future fine-tuning possibilities | `donut-base` | Clean base, no Korean bias      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047db6d",
   "metadata": {},
   "source": [
    "# so i have to prepare json data on my documents for fine tuning donut and for preparing json i will be use label studio am i wright?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9e83a",
   "metadata": {},
   "source": [
    "‚úÖ Yes, you are absolutely **right**.\n",
    "\n",
    "To **fine-tune Donut** on your English invoices, you need to prepare **image + JSON-formatted label pairs**, and using **Label Studio** is one of the **best tools** to do that.\n",
    "\n",
    "\n",
    "\n",
    "## üß© Finalized Labeling Workflow\n",
    "\n",
    "### ‚úÖ What You Need:\n",
    "- ‚úÖ **Input**: Invoice PDFs (scanned or digital)\n",
    "- ‚úÖ **Output**: Image of invoice + structured JSON fields\n",
    "- ‚úÖ **Tool**: [Label Studio](https://labelstud.io/) for visual labeling\n",
    "\n",
    "\n",
    "\n",
    "## üõ†Ô∏è Step-by-Step: Label Studio for Donut Fine-Tuning\n",
    "\n",
    "### 1. üìÑ Convert PDF ‚Üí Image (JPEG/PNG)\n",
    "Use `pdf2image` or Adobe to convert each PDF page into a high-quality `.jpg`.\n",
    "\n",
    "\n",
    "\n",
    "### 2. üß∞ Set Up Label Studio\n",
    "\n",
    "1. Create a **new project**: e.g., `Invoice Keyword Labeling`\n",
    "2. Choose **Image classification/annotation**\n",
    "3. Customize labeling interface for JSON-style key-value tagging.\n",
    "\n",
    "üí° Example label config:\n",
    "\n",
    "```xml\n",
    "<View>\n",
    "  <Image name=\"image\" value=\"$image\"/>\n",
    "  <TextArea name=\"invoice_number\" label=\"Invoice Number\"/>\n",
    "  <TextArea name=\"invoice_date\" label=\"Invoice Date\"/>\n",
    "  <TextArea name=\"vendor\" label=\"Vendor Name\"/>\n",
    "  <TextArea name=\"amount\" label=\"Amount\"/>\n",
    "</View>\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 3. üîç Label Each Invoice\n",
    "\n",
    "- For each image, manually enter the key fields (like invoice number, date, vendor, etc.) into the form.\n",
    "- This creates a structured JSON output like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"image\": \"invoice_01.jpg\",\n",
    "  \"ground_truth\": {\n",
    "    \"invoice_number\": \"INV-2311\",\n",
    "    \"invoice_date\": \"2025-06-01\",\n",
    "    \"vendor\": \"XYZ Pvt Ltd\",\n",
    "    \"amount\": \"$1350.00\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 4. üì• Export as JSON\n",
    "\n",
    "After labeling:\n",
    "- Go to Label Studio ‚Üí **Export**\n",
    "- Choose `JSON` export with image references\n",
    "- You‚Äôll get a file like `export.json`\n",
    "\n",
    "You can then postprocess this file into the Donut-required format (i.e., flatten into Donut‚Äôs `<s_invoice>...</s_invoice>` string).\n",
    "\n",
    "\n",
    "\n",
    "### 5. üîÑ (Optional) Convert to Donut Text Format\n",
    "\n",
    "Each record should become:\n",
    "```json\n",
    "{\n",
    "  \"image\": \"invoice_01.jpg\",\n",
    "  \"text_input\": \"<s_invoice><invoice_number>INV-2311</invoice_number><invoice_date>2025-06-01</invoice_date><vendor>XYZ Pvt Ltd</vendor><amount>$1350.00</amount></s_invoice>\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ Summary:\n",
    "\n",
    "| Step                  | Tool         | Output                     |\n",
    "|-----------------------|--------------|----------------------------|\n",
    "| PDF ‚Üí Image           | `pdf2image`  | `invoice_01.jpg`, etc.     |\n",
    "| Image Annotation      | Label Studio | Field-level key-value tags |\n",
    "| Export Labeled Data   | JSON         | `image` + `ground_truth`   |\n",
    "| Convert for Donut     | Python script| `image` + `text_input`     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b0b669",
   "metadata": {},
   "source": [
    "# Donuts V/s LayoutLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61e5e9",
   "metadata": {},
   "source": [
    "## üìä LayoutLM vs Donut: Use Case Comparison\n",
    "\n",
    "| Feature / Aspect                            | **LayoutLM**                                             | **Donut**                                                             |\n",
    "| ------------------------------------------- | -------------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **Developer**                               | Microsoft                                                | NAVER AI Lab                                                          |\n",
    "| **Model Type**                              | Multimodal Transformer (Text + Layout + Optional Visual) | Vision Encoder + Language Decoder (End-to-End OCR-Free)               |\n",
    "| **Input Requirement**                       | Needs OCR output (words + bounding boxes)                | Raw scanned document image (PDF page or JPG/PNG)                      |\n",
    "| **OCR Dependency**                          | ‚úÖ Required (Tesseract, Azure Read, etc.)                 | ‚ùå Not required (has built-in vision processing)                       |\n",
    "| **Text Position Handling**                  | Uses bounding boxes to learn layout                      | Learns layout directly from image via vision encoder                  |\n",
    "| **Image Support**                           | Added in LayoutLMv2/v3 (via ResNet)                      | Native image encoder (e.g., Swin Transformer or ViT)                  |\n",
    "| **Preprocessing Complexity**                | High (OCR + coordinate extraction + label alignment)     | Low (only need document images and target JSON)                       |\n",
    "| **Labeling Format**                         | BIO-tagged text + bounding boxes                         | JSON output (end-to-end training)                                     |\n",
    "| **Best Use Cases**                          | Structured forms, text-heavy docs, OCR-rich environments | Scanned receipts, invoices, handwritten forms, low-quality scans      |\n",
    "| **Fine-tuning Dataset Requirement**         | Word-level annotations + bounding boxes                  | Paired image + target JSON (much easier to annotate via Label Studio) |\n",
    "| **Training Flexibility**                    | Requires carefully aligned tokens and positions          | Trained directly to generate final output format                      |\n",
    "| **Output Format**                           | Sequence labels (NER), classification, etc.              | JSON directly (can include nested fields and tables)                  |\n",
    "| **Performance on Scanned, Complex Layouts** | Good with OCR + layout embedding                         | Excellent on raw images, especially noisy or varied formats           |\n",
    "| **Pretrained Dataset**                      | Forms (FUNSD, CORD, etc.)                                | SynthDog (synthetic docs), CORD, receipts, invoices                   |\n",
    "| **Integration with Automation Anywhere**    | Needs OCR bot ‚Üí ML model ‚Üí JSON converter                | Convert image to base64 ‚Üí Donut model API ‚Üí get JSON                  |\n",
    "| **Speed & Simplicity**                      | Slower pipeline with more preprocessing                  | Fast and simple pipeline (image ‚Üí JSON)                               |\n",
    "| **Support for Tabular Data**                | Moderate (hard to extract table cells without rules)     | Better support for extracting tables in structured JSON               |\n",
    "| **Open Source**                             | ‚úÖ Hugging Face: LayoutLM                                 | ‚úÖ Hugging Face: Donut                                                 |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ Which is Better for Your Use Case?\n",
    "\n",
    "| Criteria                                                                    | Best Option       |\n",
    "| --------------------------------------------------------------------------- | ----------------- |\n",
    "| You have **mixed document types (scanned + digital PDFs)**                  | **Donut**         |\n",
    "| You want **simpler, end-to-end JSON output**                                | **Donut**         |\n",
    "| You already have **OCR infrastructure or labeled data with bounding boxes** | **LayoutLM**      |\n",
    "| You prefer **fast prototyping with minimal preprocessing**                  | **Donut**         |\n",
    "| You need **layout-aware text understanding from OCR**                       | **LayoutLMv2/v3** |\n",
    "| You want to handle **many document formats without prompt tuning**          | **Donut**         |\n",
    "| You prefer **transformer-only models for scalable NER pipelines**           | **LayoutLM**      |\n",
    "\n",
    "\n",
    "\n",
    "## üèÅ Summary\n",
    "\n",
    "| Use Case                                                            | Best Model       |\n",
    "| ------------------------------------------------------------------- | ---------------- |\n",
    "| PO processing from **text PDFs + structured OCR output**            | ‚úÖ **LayoutLMv2** |\n",
    "| PO processing from **scanned images / mixed formats**               | ‚úÖ **Donut**      |\n",
    "| Want **one model** that takes **image ‚Üí structured JSON**           | ‚úÖ **Donut**      |\n",
    "| Want **token-level control** or downstream NER/classification tasks | ‚úÖ **LayoutLM**   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf689e9",
   "metadata": {},
   "source": [
    "# Ollama V/s Donuts V/s LayoutLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7db19c",
   "metadata": {},
   "source": [
    "let‚Äôs evaluate **fine-tuning an Ollama-compatible model** (like a local LLM such as `llama3`, `mistral`, `phi`, etc. via [Ollama](https://ollama.com/)) **for your use case: extracting structured data from POs** ‚Äî and compare it to **LayoutLM** and **Donut**.\n",
    "\n",
    "\n",
    "\n",
    "## üß† What is Ollama?\n",
    "\n",
    "**Ollama** is a platform to run LLMs like LLaMA, Mistral, or Phi **locally**, with GPU/CPU support. These models are:\n",
    "\n",
    "* General-purpose language models (like GPT, LLaMA, etc.)\n",
    "* Text-in ‚Üí text-out (no native layout/image understanding)\n",
    "* Not layout-aware or document-specific\n",
    "\n",
    "\n",
    "\n",
    "## ‚öñÔ∏è Feasibility Comparison\n",
    "\n",
    "| Feature                                | **Ollama (LLaMA, Mistral, etc.)**           | **LayoutLM**                        | **Donut**                           |\n",
    "| -------------------------------------- | ------------------------------------------- | ----------------------------------- | ----------------------------------- |\n",
    "| **Input Type**                         | Plain text only (from OCR or PDF parsing)   | OCR text + bounding boxes           | Raw document image                  |\n",
    "| **Layout Awareness**                   | ‚ùå None                                      | ‚úÖ Full (2D positions + text)        | ‚úÖ Vision-based layout understanding |\n",
    "| **OCR Dependency**                     | ‚úÖ Must pre-process PDF/image                | ‚úÖ Required                          | ‚ùå No OCR needed                     |\n",
    "| **Fine-tuning Effort**                 | ‚ùå Hard ‚Äì requires large data + compute      | Moderate (needs labeled BIO format) | Easy ‚Äì just JSON/image pairs        |\n",
    "| **Output Format**                      | Text ‚Üí requires post-processing             | Token-wise or entity-based          | JSON out-of-the-box                 |\n",
    "| **Offline, Lightweight**               | ‚úÖ (via Ollama CLI)                          | Needs Python/transformers           | Needs GPU + vision encoder          |\n",
    "| **Suitable for structured documents?** | üö´ No layout context ‚Äî hallucination likely | ‚úÖ Yes                               | ‚úÖ Yes                               |\n",
    "| **Pretrained on documents?**           | ‚ùå No                                        | ‚úÖ Yes                               | ‚úÖ Yes                               |\n",
    "\n",
    "\n",
    "\n",
    "## üö´ Why Ollama/LLaMA-style LLMs **Are Not Ideal** for PO Extraction\n",
    "\n",
    "| Limitation                                              | Impact                                                    |\n",
    "| ------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| ‚ùå No layout context                                     | Can't understand spatial relationships in tables or forms |\n",
    "| ‚ùå Requires OCR and smart prompts                        | Extra engineering needed to align PO data                 |\n",
    "| ‚ùå Tends to hallucinate on missing or low-context fields | Bad for critical business data                            |\n",
    "| ‚ùå No native JSON/field-level output                     | Needs manual post-processing, regex, parsing              |\n",
    "| ‚ùå Not fine-tunable easily without lots of data + GPU    | Expensive and time-consuming                              |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ When to Use Ollama (LLaMA-based models)?\n",
    "\n",
    "| Use Case                                     | Use Ollama?                  |\n",
    "| -------------------------------------------- | ---------------------------- |\n",
    "| Chatbot or question-answering on plain text  | ‚úÖ Yes                        |\n",
    "| Summarization or explanations of PDF content | ‚úÖ Yes                        |\n",
    "| Structured field extraction from PO invoices | ‚ùå No ‚Äì use Donut or LayoutLM |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ Final Verdict\n",
    "\n",
    "| Scenario                                                 | Recommended Model               |\n",
    "| -------------------------------------------------------- | ------------------------------- |\n",
    "| Text-based PDFs with OCR + need for layout understanding | ‚úÖ **LayoutLMv2**                |\n",
    "| Scanned images, complex layouts, simple training format  | ‚úÖ **Donut**                     |\n",
    "| Only need a chatbot or summarizer on documents           | ‚úÖ **Ollama (Mistral / LLaMA3)** |\n",
    "| Need precise field extraction with 0 hallucination       | ‚ùå **Ollama Not Recommended**    |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
